{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34b1c3f5-d22c-406b-8c4d-bf93e9a8ca85",
   "metadata": {},
   "source": [
    "# Query Values from Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "568fb9c8-07c1-4c02-9c4f-fa29b530086c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pydantic/_internal/_fields.py:192: UserWarning: Field name \"json\" in \"MonitoringDatasetFormat\" shadows an attribute in parent \"Base\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_153/4025798834.py:25: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  dev_data_df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development Data:\n",
      "   year  month carrier airport  arr_flights  arr_del15  carrier_ct  \\\n",
      "0  2004      1      DL     PBI          650        126          21   \n",
      "1  2004      1      DL     PDX          314         61          14   \n",
      "2  2004      1      DL     PHL          513         97          27   \n",
      "3  2004      1      DL     PHX          334         78          20   \n",
      "4  2004      1      DL     PIT          217         47           8   \n",
      "\n",
      "   weather_ct  nas_ct  security_ct  ...  arr_cancelled  arr_diverted  \\\n",
      "0           6      51            1  ...              4             0   \n",
      "1           2      34            0  ...             30             3   \n",
      "2           0      51            0  ...             15             0   \n",
      "3           2      39            0  ...              3             1   \n",
      "4           0      21            0  ...              4             1   \n",
      "\n",
      "   arr_delay  carrier_delay  weather_delay  nas_delay  security_delay  \\\n",
      "0       5425            881            397       2016              15   \n",
      "1       2801            478            239       1365               0   \n",
      "2       4261           1150             16       2286               0   \n",
      "3       3400           1159            166       1295               0   \n",
      "4       1737            350             28        522               0   \n",
      "\n",
      "   late_aircraft_delay  delay_rate  on_time  \n",
      "0                 2116   19.384615        0  \n",
      "1                  719   19.426752        0  \n",
      "2                  809   18.908382        0  \n",
      "3                  780   23.353292        0  \n",
      "4                  837   21.658987        0  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_153/4025798834.py:33: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  prod_data_df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Production Data:\n",
      "   year  month carrier airport  arr_flights  arr_del15  carrier_ct  \\\n",
      "0  2016      1      AA     DFW        11956       1534         507   \n",
      "1  2016      1      AA     DTW          588         98          34   \n",
      "2  2016      1      AA     SEA          607         92          35   \n",
      "3  2016      1      AA     JFK         1595        335         117   \n",
      "4  2016      1      AA     SJC          327         59          23   \n",
      "\n",
      "   weather_ct  nas_ct  security_ct  ...  arr_cancelled  arr_diverted  \\\n",
      "0          39     452            4  ...            201             9   \n",
      "1           4      19            0  ...             13             2   \n",
      "2           5      23            0  ...             12             2   \n",
      "3          10     117            0  ...            137             6   \n",
      "4           1      14            0  ...              0             0   \n",
      "\n",
      "   arr_delay  carrier_delay  weather_delay  nas_delay  security_delay  \\\n",
      "0     106950          50027           2842      13913             167   \n",
      "1       5170           1754            312        655              16   \n",
      "2       4485           1857            198        715               0   \n",
      "3      23698           9681           1592       4371              18   \n",
      "4       2276            985             94        373               5   \n",
      "\n",
      "   late_aircraft_delay  delay_rate  on_time  \n",
      "0                40001   12.830378        0  \n",
      "1                 2433   16.666666        0  \n",
      "2                 1715   15.156508        0  \n",
      "3                 8036   21.003136        0  \n",
      "4                  819   18.042814        0  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from pyathena import connect\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Reinitialize SageMaker session\n",
    "session = boto3.session.Session()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()  # Get bucket dynamically\n",
    "\n",
    "# Retrieve stored variables\n",
    "%store -r region\n",
    "%store -r role\n",
    "%store -r s3_staging_dir\n",
    "%store -r database_name\n",
    "%store -r dev_table_name\n",
    "%store -r prod_table_name\n",
    "\n",
    "# Set up Athena connection\n",
    "conn = connect(s3_staging_dir=s3_staging_dir, region_name=region)\n",
    "\n",
    "# Query development data\n",
    "query = f\"SELECT * FROM {database_name}.{dev_table_name};\"\n",
    "dev_data_df = pd.read_sql(query, conn)\n",
    "\n",
    "# Display the first few rows of development data\n",
    "print(\"Development Data:\")\n",
    "print(dev_data_df.head())\n",
    "\n",
    "# Query production data\n",
    "query = f\"SELECT * FROM {database_name}.{prod_table_name};\"\n",
    "prod_data_df = pd.read_sql(query, conn)\n",
    "\n",
    "# Display the first few rows of production data\n",
    "print(\"Production Data:\")\n",
    "print(prod_data_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332a2011-8095-495d-ba5d-9194754d176a",
   "metadata": {},
   "source": [
    "# Preprocess Data and Cast/Convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0a70edd-f6a5-42e6-8436-99d83134abd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data Preprocessing Complete. Ready for Feature Store Upload!\n",
      "Development Data Types:\n",
      "year                            int64\n",
      "month                           int64\n",
      "carrier                string[python]\n",
      "airport                string[python]\n",
      "arr_flights                     int64\n",
      "arr_del15                       int64\n",
      "carrier_ct                      int64\n",
      "weather_ct                      int64\n",
      "nas_ct                          int64\n",
      "security_ct                     int64\n",
      "late_aircraft_ct                int64\n",
      "arr_cancelled                   int64\n",
      "arr_diverted                    int64\n",
      "arr_delay                       int64\n",
      "carrier_delay                   int64\n",
      "weather_delay                   int64\n",
      "nas_delay                       int64\n",
      "security_delay                  int64\n",
      "late_aircraft_delay             int64\n",
      "delay_rate                      int64\n",
      "on_time                         int64\n",
      "event_time                    float64\n",
      "record_id              string[python]\n",
      "dtype: object\n",
      "\n",
      "Production Data Types:\n",
      "year                            int64\n",
      "month                           int64\n",
      "carrier                string[python]\n",
      "airport                string[python]\n",
      "arr_flights                     int64\n",
      "arr_del15                       int64\n",
      "carrier_ct                      int64\n",
      "weather_ct                      int64\n",
      "nas_ct                          int64\n",
      "security_ct                     int64\n",
      "late_aircraft_ct                int64\n",
      "arr_cancelled                   int64\n",
      "arr_diverted                    int64\n",
      "arr_delay                       int64\n",
      "carrier_delay                   int64\n",
      "weather_delay                   int64\n",
      "nas_delay                       int64\n",
      "security_delay                  int64\n",
      "late_aircraft_delay             int64\n",
      "delay_rate                      int64\n",
      "on_time                         int64\n",
      "event_time                    float64\n",
      "record_id              string[python]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# ✅ Convert categorical features to `string`\n",
    "def cast_object_to_string(data_frame):\n",
    "    for label in data_frame.columns:\n",
    "        if data_frame.dtypes[label] == \"object\":\n",
    "            data_frame[label] = data_frame[label].astype(\"str\").astype(\"string\")\n",
    "\n",
    "# Apply to both development and production datasets\n",
    "cast_object_to_string(dev_data_df)\n",
    "cast_object_to_string(prod_data_df)\n",
    "\n",
    "# ✅ Convert all numeric columns to `int64` (except `event_time`)\n",
    "def convert_numeric_to_int(data_frame):\n",
    "    for col in data_frame.select_dtypes(include=['int', 'float']).columns:\n",
    "        if col not in [\"event_time\"]:\n",
    "            data_frame[col] = data_frame[col].astype(\"int64\")\n",
    "\n",
    "# Apply to both datasets\n",
    "convert_numeric_to_int(dev_data_df)\n",
    "convert_numeric_to_int(prod_data_df)\n",
    "\n",
    "# ✅ Feature Engineering: Binary 'on_time' column\n",
    "def is_on_time(row):\n",
    "    return 1 if row['arr_del15'] == 0 and row['arr_cancelled'] == 0 else 0\n",
    "\n",
    "# Apply to both datasets\n",
    "dev_data_df['on_time'] = dev_data_df.apply(is_on_time, axis=1)\n",
    "prod_data_df['on_time'] = prod_data_df.apply(is_on_time, axis=1)\n",
    "\n",
    "# ✅ Ensure `event_time` is a FLOAT UNIX timestamp\n",
    "current_time_sec = int(round(time.time()))\n",
    "\n",
    "dev_data_df['event_time'] = pd.Series([current_time_sec] * len(dev_data_df), dtype=\"float64\")\n",
    "prod_data_df['event_time'] = pd.Series([current_time_sec] * len(prod_data_df), dtype=\"float64\")\n",
    "\n",
    "# ✅ Ensure `record_id` is a unique string identifier\n",
    "dev_data_df['record_id'] = dev_data_df.index.astype(\"string\")\n",
    "prod_data_df['record_id'] = prod_data_df.index.astype(\"string\")\n",
    "\n",
    "print(\"✅ Data Preprocessing Complete. Ready for Feature Store Upload!\")\n",
    "print(\"Development Data Types:\")\n",
    "print(dev_data_df.dtypes)\n",
    "print(\"\\nProduction Data Types:\")\n",
    "print(prod_data_df.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7671742a-75af-43d4-b59b-135a117875aa",
   "metadata": {},
   "source": [
    "# Create Feature Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9d52aba-2689-4214-905d-68b7cc171391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Checking if Feature Group 'airline_delay_features_dev' exists...\n",
      "✅ Feature Group 'airline_delay_features_dev' does not exist. No deletion needed.\n",
      "🔍 Checking if Feature Group 'airline_delay_features_prod' exists...\n",
      "✅ Feature Group 'airline_delay_features_prod' does not exist. No deletion needed.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import time\n",
    "\n",
    "# Set AWS region\n",
    "region = \"us-east-1\"  # Update if needed\n",
    "\n",
    "# Feature Group names for Development and Production\n",
    "dev_feature_group_name = \"airline_delay_features_dev\"\n",
    "prod_feature_group_name = \"airline_delay_features_prod\"\n",
    "\n",
    "# Initialize SageMaker client\n",
    "sagemaker_client = boto3.client('sagemaker', region_name=region)\n",
    "\n",
    "def delete_feature_group(feature_group_name):\n",
    "    \"\"\"Deletes a feature group if it exists.\"\"\"\n",
    "    try:\n",
    "        print(f\"🔍 Checking if Feature Group '{feature_group_name}' exists...\")\n",
    "\n",
    "        # Check if the feature group exists\n",
    "        existing_groups = sagemaker_client.list_feature_groups()['FeatureGroupSummaries']\n",
    "        existing_group_names = [fg['FeatureGroupName'] for fg in existing_groups]\n",
    "\n",
    "        if feature_group_name in existing_group_names:\n",
    "            print(f\"🚀 Feature Group '{feature_group_name}' found. Deleting...\")\n",
    "\n",
    "            # Delete the feature group\n",
    "            sagemaker_client.delete_feature_group(FeatureGroupName=feature_group_name)\n",
    "\n",
    "            # Wait for deletion to complete\n",
    "            while True:\n",
    "                existing_groups = sagemaker_client.list_feature_groups()['FeatureGroupSummaries']\n",
    "                existing_group_names = [fg['FeatureGroupName'] for fg in existing_groups]\n",
    "\n",
    "                if feature_group_name not in existing_group_names:\n",
    "                    print(f\"✅ Feature Group '{feature_group_name}' deleted successfully.\")\n",
    "                    break\n",
    "\n",
    "                print(\"⏳ Waiting for Feature Group deletion...\")\n",
    "                time.sleep(5)\n",
    "\n",
    "        else:\n",
    "            print(f\"✅ Feature Group '{feature_group_name}' does not exist. No deletion needed.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error deleting Feature Group '{feature_group_name}': {e}\")\n",
    "\n",
    "# ✅ Delete and recreate feature groups for both Development and Production\n",
    "delete_feature_group(dev_feature_group_name)\n",
    "delete_feature_group(prod_feature_group_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "803938a7-7559-4c38-a742-91cbbd91ee4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Feature Group 'airline_delay_features_dev' does NOT exist! Creating it now.\n",
      "✅ Feature Group 'airline_delay_features_dev' created successfully.\n",
      "⏳ Waiting for Feature Group 'airline_delay_features_dev' to become active...\n",
      "⏳ Current Feature Group 'airline_delay_features_dev' status: Creating\n",
      "⏳ Current Feature Group 'airline_delay_features_dev' status: Creating\n",
      "⏳ Current Feature Group 'airline_delay_features_dev' status: Creating\n",
      "⏳ Current Feature Group 'airline_delay_features_dev' status: Created\n",
      "✅ Feature Group 'airline_delay_features_dev' is now fully ready!\n",
      "⏳ Waiting an additional 60 seconds for stability...\n",
      "🚀 Feature Group 'airline_delay_features_prod' does NOT exist! Creating it now.\n",
      "✅ Feature Group 'airline_delay_features_prod' created successfully.\n",
      "⏳ Waiting for Feature Group 'airline_delay_features_prod' to become active...\n",
      "⏳ Current Feature Group 'airline_delay_features_prod' status: Creating\n",
      "⏳ Current Feature Group 'airline_delay_features_prod' status: Creating\n",
      "⏳ Current Feature Group 'airline_delay_features_prod' status: Creating\n",
      "⏳ Current Feature Group 'airline_delay_features_prod' status: Creating\n",
      "⏳ Current Feature Group 'airline_delay_features_prod' status: Creating\n",
      "⏳ Current Feature Group 'airline_delay_features_prod' status: Creating\n",
      "⏳ Current Feature Group 'airline_delay_features_prod' status: Created\n",
      "✅ Feature Group 'airline_delay_features_prod' is now fully ready!\n",
      "⏳ Waiting an additional 60 seconds for stability...\n",
      "Stored 'dev_feature_group_name' (str)\n",
      "Stored 'prod_feature_group_name' (str)\n",
      "Stored 'dev_s3_uri' (str)\n",
      "Stored 'prod_s3_uri' (str)\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import time\n",
    "\n",
    "# Define the Feature Group names\n",
    "dev_feature_group_name = \"airline_delay_features_dev\"\n",
    "prod_feature_group_name = \"airline_delay_features_prod\"\n",
    "\n",
    "# Initialize SageMaker client\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "# Function to create a feature group\n",
    "def create_feature_group(feature_group_name, data_df, s3_uri):\n",
    "    \"\"\"Creates a feature group if it does not exist.\"\"\"\n",
    "    \n",
    "    # ✅ Step 1: Check if Feature Group Exists\n",
    "    existing_groups = sagemaker_client.list_feature_groups()['FeatureGroupSummaries']\n",
    "    existing_group_names = [fg['FeatureGroupName'] for fg in existing_groups]\n",
    "\n",
    "    if feature_group_name in existing_group_names:\n",
    "        print(f\"✅ Feature Group '{feature_group_name}' already exists.\")\n",
    "    else:\n",
    "        print(f\"🚀 Feature Group '{feature_group_name}' does NOT exist! Creating it now.\")\n",
    "\n",
    "        # ✅ Step 2: Define Feature Group Schema\n",
    "        feature_group_definition = {\n",
    "            \"FeatureGroupName\": feature_group_name,\n",
    "            \"RecordIdentifierFeatureName\": \"record_id\",\n",
    "            \"EventTimeFeatureName\": \"event_time\",\n",
    "            \"FeatureDefinitions\": [\n",
    "                {\"FeatureName\": \"event_time\", \"FeatureType\": \"Fractional\"}\n",
    "            ] + [\n",
    "                {\n",
    "                    \"FeatureName\": col,\n",
    "                    \"FeatureType\": \"String\" if data_df[col].dtype == \"string\" else \"Integral\"\n",
    "                }\n",
    "                for col in data_df.columns if col != \"event_time\"\n",
    "            ],\n",
    "            \"OnlineStoreConfig\": {\"EnableOnlineStore\": True},\n",
    "            \"OfflineStoreConfig\": {\n",
    "                \"S3StorageConfig\": {\"S3Uri\": s3_uri},\n",
    "                \"DisableGlueTableCreation\": False,\n",
    "            },\n",
    "            \"RoleArn\": role,\n",
    "        }\n",
    "\n",
    "        # ✅ Step 3: Create Feature Group\n",
    "        try:\n",
    "            sagemaker_client.create_feature_group(**feature_group_definition)\n",
    "            print(f\"✅ Feature Group '{feature_group_name}' created successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error creating Feature Group '{feature_group_name}': {e}\")\n",
    "            return\n",
    "    \n",
    "    # ✅ Step 4: Wait Until Feature Group is Ready\n",
    "    print(f\"⏳ Waiting for Feature Group '{feature_group_name}' to become active...\")\n",
    "    while True:\n",
    "        try:\n",
    "            status_response = sagemaker_client.describe_feature_group(FeatureGroupName=feature_group_name)\n",
    "            status = status_response[\"FeatureGroupStatus\"]\n",
    "            print(f\"⏳ Current Feature Group '{feature_group_name}' status: {status}\")\n",
    "\n",
    "            if status == \"Created\":\n",
    "                print(f\"✅ Feature Group '{feature_group_name}' is now fully ready!\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error checking Feature Group '{feature_group_name}' status: {e}\")\n",
    "        \n",
    "        time.sleep(5)\n",
    "\n",
    "    print(\"⏳ Waiting an additional 60 seconds for stability...\")\n",
    "    time.sleep(60)\n",
    "\n",
    "# Define S3 locations\n",
    "dev_s3_uri = f\"s3://{bucket}/feature-store/dev/\"\n",
    "prod_s3_uri = f\"s3://{bucket}/feature-store/prod/\"\n",
    "\n",
    "# ✅ Create Feature Groups for Dev & Prod\n",
    "create_feature_group(dev_feature_group_name, dev_data_df, dev_s3_uri)\n",
    "create_feature_group(prod_feature_group_name, prod_data_df, prod_s3_uri)\n",
    "\n",
    "%store dev_feature_group_name prod_feature_group_name\n",
    "%store dev_s3_uri prod_s3_uri\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ddd28f-f6fd-41f8-97bb-befca987a06e",
   "metadata": {},
   "source": [
    "# Record insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c313ba67-faef-40e2-bc7f-c91ffee45a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Waiting for the Feature Group 'airline_delay_features_dev' to be available in Glue...\n",
      "✅ Feature Group 'airline_delay_features_dev' is now active!\n",
      "✅ Feature Store table registered in Glue for 'airline_delay_features_dev': airline_delay_features_dev_1739939876\n",
      "⏳ Waiting for the Feature Group 'airline_delay_features_prod' to be available in Glue...\n",
      "✅ Feature Group 'airline_delay_features_prod' is now active!\n",
      "✅ Feature Store table registered in Glue for 'airline_delay_features_prod': airline_delay_features_prod_1739939953\n",
      "Stored 'dev_feature_store_table' (str)\n",
      "Stored 'prod_feature_store_table' (str)\n",
      "\n",
      "✅ Stored feature store table names for development and production: airline_delay_features_dev_1739939876 airline_delay_features_prod_1739939953\n",
      "\n",
      "Stored variables and their in-db values:\n",
      "account_id                               -> '607916531205'\n",
      "create_base_csv_athena_db                -> True\n",
      "create_base_csv_athena_table             -> True\n",
      "database_name                            -> 'db_airline_delay_cause'\n",
      "dev_feature_group_name                   -> 'airline_delay_features_dev'\n",
      "dev_feature_store_table                  -> 'airline_delay_features_dev_1739939876'\n",
      "dev_s3_path                              -> 's3://sagemaker-us-east-1-607916531205/data/develo\n",
      "dev_s3_uri                               -> 's3://sagemaker-us-east-1-607916531205/feature-sto\n",
      "dev_table_name                           -> 'development_data'\n",
      "packages_installed                       -> True\n",
      "prod_feature_group_name                  -> 'airline_delay_features_prod'\n",
      "prod_feature_store_table                 -> 'airline_delay_features_prod_1739939953'\n",
      "prod_s3_path                             -> 's3://sagemaker-us-east-1-607916531205/data/produc\n",
      "prod_s3_uri                              -> 's3://sagemaker-us-east-1-607916531205/feature-sto\n",
      "prod_table_name                          -> 'production_data'\n",
      "raw_table_name                           -> 'airline_delay_cause_csv_raw'\n",
      "region                                   -> 'us-east-1'\n",
      "role                                     -> 'arn:aws:iam::607916531205:role/LabRole'\n",
      "s3_csv_private_path                      -> 's3://sagemaker-us-east-1-607916531205/airline-del\n",
      "s3_staging_dir                           -> 's3://sagemaker-us-east-1-607916531205/athena/stag\n",
      "setup_s3_bucket_passed                   -> True\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import time\n",
    "\n",
    "# Initialize SageMaker client\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "# ✅ Function to Get the Correct Offline Table Name\n",
    "def get_feature_store_table_name(feature_group_name):\n",
    "    print(f\"⏳ Waiting for the Feature Group '{feature_group_name}' to be available in Glue...\")\n",
    "\n",
    "    # Wait for Feature Group to be created\n",
    "    while True:\n",
    "        response = sagemaker_client.describe_feature_group(FeatureGroupName=feature_group_name)\n",
    "        status = response[\"FeatureGroupStatus\"]\n",
    "        \n",
    "        if status == \"Created\":\n",
    "            print(f\"✅ Feature Group '{feature_group_name}' is now active!\")\n",
    "            break\n",
    "        \n",
    "        print(f\"⏳ Current status: {status}, retrying in 5 seconds...\")\n",
    "        time.sleep(5)\n",
    "    \n",
    "    # Retrieve Glue Table Name from Offline Store Config\n",
    "    try:\n",
    "        table_name = response[\"OfflineStoreConfig\"][\"DataCatalogConfig\"][\"TableName\"]\n",
    "        print(f\"✅ Feature Store table registered in Glue for '{feature_group_name}': {table_name}\")\n",
    "        return table_name\n",
    "    except KeyError:\n",
    "        print(f\"❌ Error: Offline Store is not properly configured for '{feature_group_name}'.\")\n",
    "        return None\n",
    "\n",
    "# ✅ Get Feature Store Table Names for Dev and Prod\n",
    "dev_feature_store_table = get_feature_store_table_name(dev_feature_group_name)\n",
    "prod_feature_store_table = get_feature_store_table_name(prod_feature_group_name)\n",
    "\n",
    "# ✅ Store for use in other notebooks\n",
    "%store dev_feature_store_table prod_feature_store_table\n",
    "print()\n",
    "\n",
    "print(f\"✅ Stored feature store table names for development and production: {dev_feature_store_table} {prod_feature_store_table}\")\n",
    "\n",
    "print()\n",
    "\n",
    "%store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2070bd0-639f-43cb-b911-f5d52dd0e38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully inserted one record into Feature Store 'airline_delay_features_dev'!\n",
      "✅ Successfully inserted one record into Feature Store 'airline_delay_features_prod'!\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "import boto3\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "\n",
    "# Initialize Feature Store Runtime client\n",
    "featurestore_runtime = boto3.client('sagemaker-featurestore-runtime', region_name=\"us-east-1\")\n",
    "\n",
    "# ✅ Function to Insert a Single Row into a Feature Group\n",
    "def insert_single_record(feature_group_name, data_df):\n",
    "    \"\"\" Inserts a single row into the specified feature group. \"\"\"\n",
    "\n",
    "    # ✅ Convert event_time to UNIX timestamp (float64)\n",
    "    data_df[\"event_time\"] = time.time()\n",
    "\n",
    "    # ✅ Select one row and convert to Feature Store format\n",
    "    single_record = data_df.iloc[0].to_dict()\n",
    "\n",
    "    # ✅ Ensure event_time and record_id are included in correct format\n",
    "    record = {\n",
    "        \"FeatureGroupName\": feature_group_name,\n",
    "        \"Record\": [\n",
    "            {\"FeatureName\": key, \"ValueAsString\": str(value)} for key, value in single_record.items()\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # ✅ Insert single record into Feature Store\n",
    "    try:\n",
    "        featurestore_runtime.put_record(**record)\n",
    "        print(f\"✅ Successfully inserted one record into Feature Store '{feature_group_name}'!\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error inserting record into '{feature_group_name}': {e}\")\n",
    "\n",
    "# ✅ Insert into Development Feature Store\n",
    "insert_single_record(dev_feature_group_name, dev_data_df)\n",
    "\n",
    "# ✅ Insert into Production Feature Store\n",
    "insert_single_record(prod_feature_group_name, prod_data_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4e5446-05bc-4d86-984b-38a5b5730076",
   "metadata": {},
   "source": [
    "# Inserting All Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ea9f6eb-fcde-4513-b215-858beeea37af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 All records successfully ingested into Feature Store 'airline_delay_features_dev'!\n",
      "🚀 All records successfully ingested into Feature Store 'airline_delay_features_prod'!\n"
     ]
    }
   ],
   "source": [
    "# ✅ Initialize Feature Group objects for both Dev & Prod\n",
    "dev_feature_group = FeatureGroup(name=dev_feature_group_name, sagemaker_session=sagemaker_session)\n",
    "prod_feature_group = FeatureGroup(name=prod_feature_group_name, sagemaker_session=sagemaker_session)\n",
    "\n",
    "# ✅ Function to perform bulk ingestion into a Feature Store\n",
    "def bulk_ingest(feature_group, data_df):\n",
    "    \"\"\" Ingests data in bulk into the specified Feature Group. \"\"\"\n",
    "    try:\n",
    "        feature_group.ingest(\n",
    "            data_frame=data_df[1:],  # Use full dataset except the first row (since it was inserted earlier)\n",
    "            max_workers=5,  # Controls parallel processing\n",
    "            wait=True  # Waits for the ingestion to complete\n",
    "        )\n",
    "        print(f\"🚀 All records successfully ingested into Feature Store '{feature_group.name}'!\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during bulk ingestion into '{feature_group.name}': {e}\")\n",
    "\n",
    "# ✅ Bulk Ingest Development Data\n",
    "bulk_ingest(dev_feature_group, dev_data_df)\n",
    "\n",
    "# ✅ Bulk Ingest Production Data\n",
    "bulk_ingest(prod_feature_group, prod_data_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f709b47-cab0-4ecc-a321-87b237665462",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_153/1260608323.py:11: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  record_count_df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Development - Records in Feature Store: 130507, Total Expected: 130507\n",
      "Note: If numbers don't match, uploads are likely still completing. Wait for a minute before running this cell again.\n",
      "\n",
      "🔍 Production - Records in Feature Store: 102182, Total Expected: 102182\n",
      "Note: If numbers don't match, uploads are likely still completing. Wait for a minute before running this cell again.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ✅ Function to check record count in Feature Store\n",
    "def check_feature_store_count(feature_store_table, data_df, label):\n",
    "    \"\"\" Queries Athena to get the record count for a given feature store table. \"\"\"\n",
    "\n",
    "    query = f\"\"\"\n",
    "    SELECT COUNT(*) FROM \"sagemaker_featurestore\".\"{feature_store_table}\";\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # ✅ Execute Athena query using Pandas\n",
    "        record_count_df = pd.read_sql(query, conn)\n",
    "\n",
    "        # ✅ Print the current number of records uploaded\n",
    "        print(f\"🔍 {label} - Records in Feature Store: {record_count_df.iloc[0, 0]}, Total Expected: {len(data_df)}\")\n",
    "        print(f\"Note: If numbers don't match, uploads are likely still completing. Wait for a minute before running this cell again.\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error querying {label} Feature Store: {e}\")\n",
    "\n",
    "# ✅ Check Dev Feature Store Record Count\n",
    "check_feature_store_count(dev_feature_store_table, dev_data_df, \"Development\")\n",
    "\n",
    "# ✅ Check Prod Feature Store Record Count\n",
    "check_feature_store_count(prod_feature_store_table, prod_data_df, \"Production\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e49225-4adb-4f1d-a88f-1bcf27289fd1",
   "metadata": {},
   "source": [
    "***Note!!! Seems like when we push one entry in, sometimes it gets stuck. Executing the command to push all the queries in seems to force things down into the feature store. I moved the reading of the single entry to the end, even though it was originally designed to read the item that we had inserted into the stack before we executed the bulk processing. Hope this makes sense***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef954f8c-9b00-4833-8876-da153f9783c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_153/2926717501.py:17: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  offline_record_df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Development - Record Found in Feature Store (Offline Store via Athena):\n",
      "     event_time  year  month carrier airport  arr_flights  arr_del15  \\\n",
      "0  1.739940e+09  2004      1      DL     PBI          650        126   \n",
      "\n",
      "   carrier_ct  weather_ct  nas_ct  ...  weather_delay  nas_delay  \\\n",
      "0          21           6      51  ...            397       2016   \n",
      "\n",
      "   security_delay  late_aircraft_delay  delay_rate  on_time  record_id  \\\n",
      "0              15                 2116          19        0          0   \n",
      "\n",
      "               write_time  api_invocation_time  is_deleted  \n",
      "0 2025-02-19 04:45:48.072  2025-02-19 04:40:56       False  \n",
      "\n",
      "[1 rows x 26 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_153/2926717501.py:17: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  offline_record_df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Production - Record Found in Feature Store (Offline Store via Athena):\n",
      "     event_time  year  month carrier airport  arr_flights  arr_del15  \\\n",
      "0  1.739940e+09  2016      1      AA     DFW        11956       1534   \n",
      "\n",
      "   carrier_ct  weather_ct  nas_ct  ...  weather_delay  nas_delay  \\\n",
      "0         507          39     452  ...           2842      13913   \n",
      "\n",
      "   security_delay  late_aircraft_delay  delay_rate  on_time  record_id  \\\n",
      "0             167                40001          12        0          0   \n",
      "\n",
      "               write_time  api_invocation_time  is_deleted  \n",
      "0 2025-02-19 04:45:47.571  2025-02-19 04:40:56       False  \n",
      "\n",
      "[1 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "# ✅ Ensure we have the correct `record_id` from the inserted data\n",
    "dev_record_id = dev_data_df.iloc[0][\"record_id\"]\n",
    "prod_record_id = prod_data_df.iloc[0][\"record_id\"]\n",
    "\n",
    "# ✅ Function to query a record from Offline Store (Athena)\n",
    "def query_feature_store_record(feature_store_table, record_id, label):\n",
    "    \"\"\" Queries Athena to retrieve a specific record from the Feature Store offline store. \"\"\"\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    SELECT * FROM \"sagemaker_featurestore\".\"{feature_store_table}\"\n",
    "    WHERE record_id = '{record_id}'\n",
    "    LIMIT 1;\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # ✅ Execute the query using Pandas\n",
    "        offline_record_df = pd.read_sql(query, conn)\n",
    "\n",
    "        # ✅ Check if the record exists\n",
    "        if not offline_record_df.empty:\n",
    "            print(f\"✅ {label} - Record Found in Feature Store (Offline Store via Athena):\")\n",
    "            print(offline_record_df)\n",
    "        else:\n",
    "            print(f\"❌ {label} - Record not found in Offline Store!\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error querying {label} Feature Store: {e}\")\n",
    "\n",
    "# ✅ Query the inserted record in Dev Feature Store\n",
    "query_feature_store_record(dev_feature_store_table, dev_record_id, \"Development\")\n",
    "\n",
    "# ✅ Query the inserted record in Prod Feature Store\n",
    "query_feature_store_record(prod_feature_store_table, prod_record_id, \"Production\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae6f560-cbc2-4392-be18-557f7b89996f",
   "metadata": {},
   "source": [
    "# Release Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54a32ee7-bc9e-4cf9-b395-6dd0e70a5fbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<p><b>Shutting down your kernel for this notebook to release resources.</b></p>\n",
       "<button class=\"sm-command-button\" data-commandlinker-command=\"kernelmenu:shutdown\" style=\"display:none;\">Shutdown Kernel</button>\n",
       "        \n",
       "<script>\n",
       "try {\n",
       "    els = document.getElementsByClassName(\"sm-command-button\");\n",
       "    els[0].click();\n",
       "}\n",
       "catch(err) {\n",
       "    // NoOp\n",
       "}    \n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "\n",
    "<p><b>Shutting down your kernel for this notebook to release resources.</b></p>\n",
    "<button class=\"sm-command-button\" data-commandlinker-command=\"kernelmenu:shutdown\" style=\"display:none;\">Shutdown Kernel</button>\n",
    "        \n",
    "<script>\n",
    "try {\n",
    "    els = document.getElementsByClassName(\"sm-command-button\");\n",
    "    els[0].click();\n",
    "}\n",
    "catch(err) {\n",
    "    // NoOp\n",
    "}    \n",
    "</script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972d6d0b-5fcb-4ac2-92f7-45e7f53507a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "\n",
    "try {\n",
    "    Jupyter.notebook.save_checkpoint();\n",
    "    Jupyter.notebook.session.delete();\n",
    "}\n",
    "catch(err) {\n",
    "    // NoOp\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
