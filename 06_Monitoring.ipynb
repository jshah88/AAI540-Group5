{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61c1f5cf-db15-4b28-907e-07ddcd8223e7",
   "metadata": {},
   "source": [
    "### Data Capture Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58939eb3-761c-4372-bec1-6d024b146ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pydantic/_internal/_fields.py:192: UserWarning: Field name \"json\" in \"MonitoringDatasetFormat\" shadows an attribute in parent \"Base\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "⚠️ No JSON files found. Send data to the endpoint for capture.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "# ✅ Retrieve stored variables\n",
    "%store -r endpoint_name_single_request\n",
    "%store -r s3_staging_dir\n",
    "\n",
    "# ✅ Initialize AWS session\n",
    "session = boto3.session.Session()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# ✅ Reinitialize `bucket`\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "# ✅ Initialize S3 client\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "# ✅ Define S3 prefix for captured data dynamically\n",
    "data_capture_prefix = f\"data_capture/{endpoint_name_single_request}/AllTraffic/\"\n",
    "\n",
    "# ✅ List objects in the S3 data capture folder\n",
    "capture_files = s3_client.list_objects_v2(Bucket=bucket, Prefix=data_capture_prefix)\n",
    "\n",
    "if \"Contents\" in capture_files:\n",
    "    print(\"✅ JSON files available in data capture S3 path:\")\n",
    "    for obj in capture_files[\"Contents\"]:\n",
    "        print(f\"- {obj['Key']}\")\n",
    "else:\n",
    "    print(\"⚠️ No JSON files found. Send data to the endpoint for capture.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cbcef7-8a50-4aaf-a454-160ee97ed091",
   "metadata": {},
   "source": [
    "### Take sample from validation data to send to the predictor endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eca2e432-425c-4504-a6a2-a29cf0810d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0             1     2   3   4    5     6    7    8   9   ...   12  13  14  \\\n",
      "0   0  1.739940e+09  2011   4   1  202    62    5    2   0  ...    0   0   1   \n",
      "1   0  1.739940e+09  2005   8   6   60    93   17   11   0  ...    5   0   0   \n",
      "2   0  1.739940e+09  2015   4   6   92  3848  602  108  13  ...  279  78  20   \n",
      "3   0  1.739940e+09  2007  12   0    2   284  100   32   9  ...   32   7   1   \n",
      "4   0  1.739940e+09  2004  10   6  194  4127  555  241  85  ...  179  67   2   \n",
      "\n",
      "      15     16    17    18  19     20  21  \n",
      "0    176    106     0    64   0      6   8  \n",
      "1    513    323     0    28   0    162  18  \n",
      "2  37754  11074  1273  6656  10  18741  15  \n",
      "3   5876   1641   533   947   0   2755  35  \n",
      "4  25239   9236  6015  2344  19   7625  13  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "Validation data shape: (26102, 22)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load validation.csv\n",
    "validation_file = \"validation.csv\"\n",
    "validation_data = pd.read_csv(validation_file, header=None)\n",
    "\n",
    "# Display the first few rows to confirm the structure\n",
    "print(validation_data.head())\n",
    "print(f\"Validation data shape: {validation_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62f23b84-0edc-4c8e-be0e-dc9d5776c3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 50 rows for sending to endpoint.\n"
     ]
    }
   ],
   "source": [
    "# Select a subset of validation data\n",
    "subset_size = 50  # Number of rows to send to the endpoint\n",
    "subset_data = validation_data.iloc[:subset_size, 1:]  # Exclude the first column (target variable)\n",
    "\n",
    "# Convert to CSV-like string for SageMaker endpoint\n",
    "subset_data_str_list = subset_data.apply(lambda row: \",\".join(row.astype(str)), axis=1).tolist()\n",
    "print(f\"Prepared {len(subset_data_str_list)} rows for sending to endpoint.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e7d944-abc4-4228-96a0-ef74e46ef7e9",
   "metadata": {},
   "source": [
    "### Create predictor endpoint and send data to endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f639d32-0a9e-4b4f-98cc-47b10dceed5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 1739940056.2871292,2011.0,4.0,1.0,202.0,62.0,5.0,2.0,0.0,2.0,0.0,0.0,0.0,1.0,176.0,106.0,0.0,64.0,0.0,6.0,8.0\n",
      "Prediction: {'predictions': [{'score': 1.0493714398762677e-05}]}\n",
      "Input: 1739940056.2871292,2005.0,8.0,6.0,60.0,93.0,17.0,11.0,0.0,1.0,0.0,5.0,0.0,0.0,513.0,323.0,0.0,28.0,0.0,162.0,18.0\n",
      "Prediction: {'predictions': [{'score': 1.0493714398762677e-05}]}\n",
      "Input: 1739940056.2871292,2015.0,4.0,6.0,92.0,3848.0,602.0,108.0,13.0,200.0,0.0,279.0,78.0,20.0,37754.0,11074.0,1273.0,6656.0,10.0,18741.0,15.0\n",
      "Prediction: {'predictions': [{'score': 4.370238002593396e-06}]}\n",
      "Input: 1739940056.2871292,2007.0,12.0,0.0,2.0,284.0,100.0,32.0,9.0,25.0,0.0,32.0,7.0,1.0,5876.0,1641.0,533.0,947.0,0.0,2755.0,35.0\n",
      "Prediction: {'predictions': [{'score': 4.370238002593396e-06}]}\n",
      "Input: 1739940056.2871292,2004.0,10.0,6.0,194.0,4127.0,555.0,241.0,85.0,49.0,1.0,179.0,67.0,2.0,25239.0,9236.0,6015.0,2344.0,19.0,7625.0,13.0\n",
      "Prediction: {'predictions': [{'score': 4.370238002593396e-06}]}\n",
      "Input: 1739940056.2871292,2011.0,8.0,9.0,172.0,576.0,82.0,32.0,3.0,9.0,0.0,36.0,4.0,0.0,4107.0,1489.0,149.0,399.0,17.0,2053.0,14.0\n",
      "Prediction: {'predictions': [{'score': 4.370238002593396e-06}]}\n",
      "Input: 1739940056.2871292,2009.0,9.0,6.0,65.0,7.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "Prediction: {'predictions': [{'score': 0.005108979064971209}]}\n",
      "Input: 1739940056.2871292,2008.0,3.0,0.0,251.0,212.0,58.0,22.0,5.0,9.0,0.0,20.0,17.0,1.0,3619.0,1138.0,478.0,609.0,0.0,1394.0,27.0\n",
      "Prediction: {'predictions': [{'score': 4.370238002593396e-06}]}\n",
      "Input: 1739940056.2871292,2004.0,4.0,6.0,311.0,240.0,32.0,15.0,2.0,7.0,0.0,8.0,0.0,0.0,1426.0,833.0,58.0,272.0,0.0,263.0,13.0\n",
      "Prediction: {'predictions': [{'score': 1.0493714398762677e-05}]}\n",
      "Input: 1739940056.2871292,2007.0,2.0,8.0,327.0,38.0,16.0,4.0,0.0,3.0,0.0,8.0,1.0,1.0,758.0,195.0,41.0,103.0,0.0,419.0,42.0\n",
      "Prediction: {'predictions': [{'score': 4.370238002593396e-06}]}\n",
      "Input: 1739940056.2871292,2012.0,6.0,7.0,2.0,35.0,10.0,5.0,0.0,0.0,0.0,4.0,1.0,0.0,842.0,356.0,0.0,28.0,0.0,458.0,28.0\n",
      "Prediction: {'predictions': [{'score': 4.370238002593396e-06}]}\n",
      "Input: 1739940056.2871292,2012.0,10.0,5.0,129.0,120.0,19.0,11.0,0.0,4.0,0.0,3.0,0.0,0.0,807.0,431.0,34.0,184.0,0.0,158.0,15.0\n",
      "Prediction: {'predictions': [{'score': 1.0493714398762677e-05}]}\n",
      "Input: 1739940056.2871292,2011.0,12.0,4.0,307.0,29.0,3.0,1.0,0.0,0.0,0.0,1.0,2.0,0.0,152.0,51.0,0.0,1.0,0.0,100.0,10.0\n",
      "Prediction: {'predictions': [{'score': 4.8521446842642035e-06}]}\n",
      "Input: 1739940056.2871292,2005.0,11.0,8.0,41.0,218.0,22.0,4.0,0.0,7.0,0.0,9.0,2.0,1.0,932.0,151.0,0.0,396.0,0.0,385.0,10.0\n",
      "Prediction: {'predictions': [{'score': 4.370238002593396e-06}]}\n",
      "Input: 1739940056.2871292,2005.0,5.0,6.0,358.0,120.0,10.0,10.0,0.0,0.0,0.0,0.0,0.0,0.0,342.0,342.0,0.0,0.0,0.0,0.0,8.0\n",
      "Prediction: {'predictions': [{'score': 1.0493714398762677e-05}]}\n",
      "Input: 1739940056.2871292,2007.0,4.0,6.0,66.0,116.0,27.0,16.0,0.0,0.0,0.0,11.0,1.0,0.0,1175.0,613.0,0.0,0.0,0.0,562.0,23.0\n",
      "Prediction: {'predictions': [{'score': 4.370238002593396e-06}]}\n",
      "Input: 1739940056.2871292,2009.0,7.0,1.0,49.0,71.0,15.0,6.0,0.0,1.0,0.0,6.0,0.0,0.0,1369.0,983.0,37.0,53.0,0.0,296.0,21.0\n",
      "Prediction: {'predictions': [{'score': 1.0493714398762677e-05}]}\n",
      "Input: 1739940056.2871292,2011.0,6.0,4.0,126.0,41.0,24.0,13.0,0.0,5.0,0.0,5.0,1.0,0.0,1133.0,755.0,0.0,120.0,0.0,258.0,58.0\n",
      "Prediction: {'predictions': [{'score': 4.370238002593396e-06}]}\n",
      "Input: 1739940056.2871292,2007.0,9.0,8.0,91.0,2020.0,285.0,61.0,5.0,141.0,1.0,74.0,25.0,1.0,11614.0,2990.0,296.0,4717.0,25.0,3586.0,14.0\n",
      "Prediction: {'predictions': [{'score': 4.370238002593396e-06}]}\n",
      "Input: 1739940056.2871292,2011.0,11.0,5.0,166.0,96.0,28.0,4.0,0.0,14.0,0.0,7.0,2.0,0.0,1026.0,315.0,21.0,402.0,0.0,288.0,29.0\n",
      "Prediction: {'predictions': [{'score': 4.370238002593396e-06}]}\n",
      "Input: 1739940056.2871292,2006.0,7.0,8.0,240.0,217.0,68.0,28.0,0.0,23.0,0.0,15.0,2.0,0.0,3205.0,1444.0,48.0,854.0,0.0,859.0,31.0\n",
      "Prediction: {'predictions': [{'score': 4.370238002593396e-06}]}\n",
      "Input: 1739940056.2871292,2012.0,10.0,1.0,187.0,307.0,37.0,15.0,4.0,6.0,0.0,10.0,3.0,3.0,2475.0,1433.0,288.0,174.0,0.0,580.0,12.0\n",
      "Prediction: {'predictions': [{'score': 4.370238002593396e-06}]}\n",
      "Input: 1739940056.2871292,2015.0,2.0,7.0,42.0,45.0,9.0,4.0,0.0,1.0,0.0,2.0,0.0,0.0,395.0,175.0,14.0,48.0,0.0,158.0,20.0\n",
      "Prediction: {'predictions': [{'score': 1.0493714398762677e-05}]}\n",
      "Input: 1739940056.2871292,2014.0,8.0,6.0,171.0,57.0,7.0,0.0,0.0,1.0,0.0,3.0,0.0,0.0,252.0,37.0,20.0,35.0,0.0,160.0,12.0\n",
      "Prediction: {'predictions': [{'score': 1.0493714398762677e-05}]}\n",
      "Input: 1739940056.2871292,2011.0,2.0,4.0,166.0,58.0,13.0,5.0,2.0,3.0,0.0,1.0,1.0,1.0,594.0,280.0,124.0,104.0,0.0,86.0,22.0\n",
      "Prediction: {'predictions': [{'score': 4.370238002593396e-06}]}\n",
      "Input: 1739940056.2871292,2010.0,3.0,2.0,43.0,2104.0,665.0,152.0,2.0,317.0,2.0,191.0,38.0,2.0,37573.0,8997.0,94.0,14688.0,46.0,13748.0,31.0\n",
      "Prediction: {'predictions': [{'score': 4.370238002593396e-06}]}\n",
      "Input: 1739940056.2871292,2010.0,6.0,0.0,329.0,932.0,233.0,65.0,11.0,62.0,0.0,93.0,21.0,3.0,13399.0,3779.0,824.0,2661.0,0.0,6135.0,25.0\n",
      "Prediction: {'predictions': [{'score': 4.370238002593396e-06}]}\n",
      "Input: 1739940056.2871292,2009.0,5.0,5.0,11.0,93.0,27.0,11.0,3.0,3.0,0.0,8.0,3.0,0.0,1310.0,390.0,304.0,157.0,0.0,459.0,29.0\n",
      "Prediction: {'predictions': [{'score': 4.370238002593396e-06}]}\n",
      "Input: 1739940056.2871292,2015.0,9.0,3.0,266.0,607.0,84.0,16.0,5.0,49.0,0.0,12.0,0.0,0.0,4604.0,611.0,500.0,2874.0,0.0,619.0,13.0\n",
      "Prediction: {'predictions': [{'score': 1.0493714398762677e-05}]}\n",
      "Input: 1739940056.2871292,2012.0,1.0,6.0,72.0,24.0,5.0,0.0,0.0,1.0,0.0,2.0,0.0,0.0,220.0,16.0,11.0,66.0,0.0,127.0,20.0\n",
      "Prediction: {'predictions': [{'score': 1.409655305906199e-05}]}\n",
      "Input: 1739940056.2871292,2012.0,4.0,7.0,70.0,1037.0,186.0,62.0,0.0,76.0,1.0,45.0,1.0,1.0,7718.0,2733.0,24.0,2038.0,74.0,2849.0,17.0\n",
      "Prediction: {'predictions': [{'score': 4.370238002593396e-06}]}\n",
      "Input: 1739940056.2871292,2005.0,8.0,3.0,299.0,399.0,85.0,31.0,1.0,23.0,0.0,28.0,0.0,0.0,4867.0,2074.0,90.0,849.0,0.0,1854.0,21.0\n",
      "Prediction: {'predictions': [{'score': 1.0493714398762677e-05}]}\n",
      "Input: 1739940056.2871292,2013.0,8.0,9.0,92.0,4995.0,1291.0,368.0,36.0,311.0,6.0,566.0,3.0,24.0,62703.0,18239.0,3780.0,12864.0,210.0,27610.0,25.0\n",
      "Prediction: {'predictions': [{'score': 4.370238002593396e-06}]}\n",
      "Input: 1739940056.2871292,2012.0,8.0,7.0,166.0,55.0,11.0,4.0,0.0,0.0,0.0,5.0,0.0,0.0,641.0,195.0,0.0,9.0,0.0,437.0,20.0\n",
      "Prediction: {'predictions': [{'score': 1.0493714398762677e-05}]}\n",
      "Input: 1739940056.2871292,2004.0,10.0,3.0,165.0,300.0,56.0,18.0,0.0,25.0,0.0,12.0,2.0,1.0,2375.0,1029.0,4.0,747.0,0.0,595.0,18.0\n",
      "Prediction: {'predictions': [{'score': 4.370238002593396e-06}]}\n",
      "Input: 1739940056.2871292,2007.0,6.0,6.0,243.0,67.0,18.0,11.0,0.0,1.0,1.0,5.0,0.0,0.0,702.0,259.0,0.0,151.0,25.0,267.0,26.0\n",
      "Prediction: {'predictions': [{'score': 1.0493714398762677e-05}]}\n",
      "Input: 1739940056.2871292,2006.0,7.0,6.0,320.0,505.0,79.0,62.0,0.0,0.0,0.0,17.0,9.0,1.0,3256.0,2205.0,0.0,0.0,0.0,1051.0,15.0\n",
      "Prediction: {'predictions': [{'score': 4.370238002593396e-06}]}\n",
      "Input: 1739940056.2871292,2007.0,7.0,3.0,300.0,151.0,67.0,17.0,0.0,19.0,0.0,28.0,0.0,0.0,4089.0,993.0,62.0,766.0,0.0,2268.0,44.0\n",
      "Prediction: {'predictions': [{'score': 1.0493714398762677e-05}]}\n",
      "Input: 1739940056.2871292,2014.0,10.0,7.0,184.0,401.0,84.0,33.0,0.0,34.0,0.0,15.0,1.0,0.0,4757.0,2083.0,24.0,1483.0,0.0,1167.0,20.0\n",
      "Prediction: {'predictions': [{'score': 4.370238002593396e-06}]}\n",
      "Input: 1739940056.2871292,2006.0,2.0,7.0,101.0,189.0,36.0,13.0,1.0,2.0,0.0,18.0,0.0,0.0,2354.0,644.0,98.0,93.0,0.0,1519.0,19.0\n",
      "Prediction: {'predictions': [{'score': 1.0493714398762677e-05}]}\n",
      "Input: 1739940056.2871292,2013.0,7.0,3.0,183.0,355.0,122.0,47.0,4.0,23.0,0.0,47.0,0.0,0.0,6470.0,2646.0,325.0,904.0,0.0,2595.0,34.0\n",
      "Prediction: {'predictions': [{'score': 1.0493714398762677e-05}]}\n",
      "Input: 1739940056.2871292,2008.0,4.0,7.0,215.0,617.0,152.0,55.0,2.0,40.0,0.0,53.0,2.0,1.0,8282.0,2999.0,135.0,1683.0,0.0,3465.0,24.0\n",
      "Prediction: {'predictions': [{'score': 4.370238002593396e-06}]}\n",
      "Input: 1739940056.2871292,2005.0,5.0,8.0,41.0,216.0,34.0,11.0,0.0,8.0,0.0,14.0,2.0,0.0,1680.0,417.0,35.0,206.0,0.0,1022.0,15.0\n",
      "Prediction: {'predictions': [{'score': 4.370238002593396e-06}]}\n",
      "Input: 1739940056.2871292,2007.0,12.0,6.0,158.0,61.0,20.0,9.0,1.0,2.0,0.0,8.0,8.0,0.0,981.0,306.0,82.0,142.0,0.0,451.0,32.0\n",
      "Prediction: {'predictions': [{'score': 4.370238002593396e-06}]}\n",
      "Input: 1739940056.2871292,2009.0,2.0,0.0,252.0,136.0,18.0,11.0,0.0,3.0,0.0,3.0,2.0,0.0,1011.0,684.0,0.0,71.0,0.0,256.0,13.0\n",
      "Prediction: {'predictions': [{'score': 4.370238002593396e-06}]}\n",
      "Input: 1739940056.2871292,2004.0,5.0,1.0,192.0,403.0,71.0,42.0,1.0,11.0,0.0,15.0,1.0,0.0,2658.0,1830.0,28.0,292.0,0.0,508.0,17.0\n",
      "Prediction: {'predictions': [{'score': 4.370238002593396e-06}]}\n",
      "Input: 1739940056.2871292,2004.0,4.0,1.0,308.0,3890.0,536.0,145.0,2.0,124.0,7.0,256.0,43.0,11.0,24312.0,6478.0,140.0,4234.0,161.0,13299.0,13.0\n",
      "Prediction: {'predictions': [{'score': 4.370238002593396e-06}]}\n",
      "Input: 1739940056.2871292,2004.0,1.0,2.0,52.0,89.0,18.0,9.0,0.0,5.0,0.0,3.0,0.0,0.0,685.0,343.0,0.0,188.0,0.0,154.0,20.0\n",
      "Prediction: {'predictions': [{'score': 1.0493714398762677e-05}]}\n",
      "Input: 1739940056.2871292,2013.0,7.0,0.0,203.0,93.0,38.0,13.0,2.0,4.0,0.0,16.0,1.0,0.0,1879.0,690.0,97.0,239.0,0.0,853.0,40.0\n",
      "Prediction: {'predictions': [{'score': 4.370238002593396e-06}]}\n",
      "Input: 1739940056.2871292,2013.0,12.0,8.0,14.0,64.0,2.0,2.0,0.0,0.0,0.0,0.0,0.0,0.0,66.0,66.0,0.0,0.0,0.0,0.0,3.0\n",
      "Prediction: {'predictions': [{'score': 1.0493714398762677e-05}]}\n",
      "✅ Finished sending all rows.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "# ✅ Retrieve stored endpoint name\n",
    "%store -r endpoint_name_single_request\n",
    "\n",
    "# ✅ Initialize the endpoint predictor dynamically\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name_single_request,  # Now using stored variable\n",
    "    sagemaker_session=sagemaker.Session(),\n",
    "    serializer=CSVSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")\n",
    "\n",
    "# ✅ Send each row to the endpoint\n",
    "responses = []\n",
    "for row_str in subset_data_str_list:\n",
    "    response = predictor.predict(row_str)  # Send to the endpoint\n",
    "    responses.append(response)\n",
    "    print(f\"Input: {row_str}\")\n",
    "    print(f\"Prediction: {response}\")\n",
    "\n",
    "print(\"✅ Finished sending all rows.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064a0d1a-c5b7-46ec-970c-620f0901d993",
   "metadata": {},
   "source": [
    "### Validate that JSONL files were captured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c2ab1ed-3539-4b03-ab1d-41bdcf524572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ No files found in S3 data capture path: data_capture/flight-delay-xgboost-endpoint-single-request/AllTraffic/\n"
     ]
    }
   ],
   "source": [
    "# ✅ Use already initialized session, bucket, and S3 client\n",
    "data_capture_prefix = f\"data_capture/{endpoint_name_single_request}/AllTraffic/\"\n",
    "\n",
    "# ✅ List files in S3 data capture path\n",
    "response = s3_client.list_objects_v2(Bucket=bucket, Prefix=data_capture_prefix)\n",
    "\n",
    "if \"Contents\" in response:\n",
    "    print(f\"✅ Files found in S3 data capture path ({data_capture_prefix}):\")\n",
    "    for obj in response[\"Contents\"]:\n",
    "        print(obj[\"Key\"])\n",
    "else:\n",
    "    print(f\"⚠️ No files found in S3 data capture path: {data_capture_prefix}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd88afee-152d-4f34-bce8-c470c2a734c1",
   "metadata": {},
   "source": [
    "#### You should now be able to see your data in cloudwatch\n",
    "- Go to Cloudwatch -> Logs -> Log Insights\n",
    "- search `/aws/sagemaker/Endpoints/flight-delay-xgboost-endpoint-single-request`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2161b400-f4df-44b0-be1e-2f3d372634c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker session initialized.\n",
      "SageMaker execution role: arn:aws:iam::450772039932:role/service-role/AmazonSageMaker-ExecutionRole-20250127T165558\n",
      "Default S3 bucket: sagemaker-us-east-2-450772039932\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import boto3\n",
    "from sagemaker import get_execution_role, Session\n",
    "\n",
    "# Initialize the SageMaker session and execution role\n",
    "sagemaker_session = Session()  # Create a SageMaker session\n",
    "role_arn = get_execution_role()  # Get the SageMaker execution role\n",
    "bucket = sagemaker_session.default_bucket()  # Use the default S3 bucket for SageMaker\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Print out the initialized variables\n",
    "print(\"SageMaker session initialized.\")\n",
    "print(\"SageMaker execution role:\", role_arn)\n",
    "print(\"Default S3 bucket:\", bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e5d4d9-2a56-4854-88dc-817e6f4b92d2",
   "metadata": {},
   "source": [
    "## Initialize Baseline Monitoring Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3226806c-7d6b-4687-81fe-e31af556b842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline dataset path: s3://sagemaker-us-east-2-450772039932/data_capture/flight-delay-xgboost-endpoint-single-request/AllTraffic/\n",
      "Baseline results URI: s3://sagemaker-us-east-2-450772039932/flight-delay-baseline-results/\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.model_monitor import DefaultModelMonitor, DatasetFormat\n",
    "# Define Required Variables and Initialize SageMaker Session\n",
    "\n",
    "# Define S3 URIs for baseline results and captured data\n",
    "baseline_results_uri = f\"s3://{bucket}/flight-delay-baseline-results/\"\n",
    "baseline_dataset_path = f\"s3://{bucket}/data_capture/flight-delay-xgboost-endpoint-single-request/AllTraffic/\"\n",
    "\n",
    "print(\"Baseline dataset path:\", baseline_dataset_path)\n",
    "print(\"Baseline results URI:\", baseline_results_uri)\n",
    "\n",
    "# Initialize the model monitor\n",
    "model_monitor = DefaultModelMonitor(\n",
    "    role=role_arn,\n",
    "    instance_type=\"ml.m5.xlarge\",  # Adjust instance type if needed\n",
    "    instance_count=1,\n",
    "    max_runtime_in_seconds=3600,  # Maximum runtime for baseline generation\n",
    "    sagemaker_session=sagemaker_session\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a1f213-b7be-4481-8d36-be431e3d919e",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">TODO - fix baseline model error</span>\n",
    "### Run baseline suggestion job\n",
    "- UnexpectedStatusException: Error for Processing job baseline-suggestion-job-2025-02-17-05-13-52-651: Failed. Reason: AlgorithmError: Error: Could not find any json file under directory /opt/ml/processing/input/baseline_dataset_input. Please verify if the provided dataset path is correct or if data capturing in your Endpoint is turned on., exit code: 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "302efffe-b973-49e5-8a11-da1913a4846c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name baseline-suggestion-job-2025-02-17-05-13-52-651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running baseline suggestion job...\n",
      "...........\u001b[34m2025-02-17 05:15:38.248570: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:38.248598: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:39.851076: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:39.851107: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:39.851126: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-0-178-129.us-east-2.compute.internal): /proc/driver/nvidia/version does not exist\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:39.851401: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\u001b[0m\n",
      "\u001b[34mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:41,443 - __main__ - INFO - All params:{'ProcessingJobArn': 'arn:aws:sagemaker:us-east-2:450772039932:processing-job/baseline-suggestion-job-2025-02-17-05-13-52-651', 'ProcessingJobName': 'baseline-suggestion-job-2025-02-17-05-13-52-651', 'Environment': {'dataset_format': '{\"json\": {\"lines\": true}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}, 'AppSpecification': {'ImageUri': '777275614652.dkr.ecr.us-east-2.amazonaws.com/sagemaker-model-monitor-analyzer', 'ContainerEntrypoint': None, 'ContainerArguments': None}, 'ProcessingInputs': [{'InputName': 'baseline_dataset_input', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/baseline_dataset_input', 'S3Uri': 's3://sagemaker-us-east-2-450772039932/data_capture/flight-delay-xgboost-endpoint-single-request/AllTraffic/', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinitionInput': None}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'monitoring_output', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/output', 'S3Uri': 's3://sagemaker-us-east-2-450772039932/flight-delay-baseline-results/', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.m5.xlarge', 'VolumeSizeInGB': 30, 'VolumeKmsKeyId': None}}, 'NetworkConfig': {'VpcConfig': None, 'EnableNetworkIsolation': False, 'EnableInterContainerTrafficEncryption': False}, 'RoleArn': 'arn:aws:iam::450772039932:role/service-role/AmazonSageMaker-ExecutionRole-20250127T165558', 'StoppingCondition': {'MaxRuntimeInSeconds': 3600}}\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:41,443 - __main__ - INFO - Current Environment:{'dataset_format': '{\"json\": {\"lines\": true}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:41,443 - __main__ - INFO - categorical_drift_method:None\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:41,443 - DefaultDataAnalyzer - INFO - Performing analysis with input: {\"dataset_source\": \"/opt/ml/processing/input/baseline_dataset_input\", \"dataset_format\": {\"json\": {\"lines\": true}}, \"output_path\": \"/opt/ml/processing/output\", \"monitoring_input_type\": null, \"analysis_type\": null, \"problem_type\": null, \"inference_attribute\": null, \"probability_attribute\": null, \"ground_truth_attribute\": null, \"probability_threshold_attribute\": null, \"positive_label\": null, \"exclude_features_attribute\": null, \"record_preprocessor_script\": null, \"post_analytics_processor_script\": null, \"baseline_constraints\": null, \"baseline_statistics\": null, \"data_quality_monitoring_config\": {\"evaluate_constraints\": \"Enabled\", \"emit_metrics\": \"Enabled\", \"datatype_check_threshold\": 1.0, \"domain_content_threshold\": 1.0, \"distribution_constraints\": {\"perform_comparison\": \"Enabled\", \"comparison_threshold\": 0.1, \"comparison_method\": \"Robust\", \"categorical_comparison_threshold\": 0.1, \"categorical_drift_method\": \"LInfinity\"}}, \"start_time\": null, \"end_time\": null, \"metric_time\": null, \"cloudwatch_metrics_directory\": \"/opt/ml/output/metrics/cloudwatch\", \"publish_cloudwatch_metrics\": \"Disabled\", \"sagemaker_endpoint_name\": null, \"sagemaker_monitoring_schedule_name\": null, \"output_message_file\": \"/opt/ml/output/message\", \"detect_outliers\": null, \"detect_drift\": null, \"image_data\": null, \"report_enabled\": false, \"auto_ml_job_detail\": null}\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:41,443 - DefaultDataAnalyzer - INFO - Bootstrapping yarn\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:41,443 - bootstrap - INFO - Copy aws jars\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:41,491 - bootstrap - INFO - Copy cluster config\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:41,492 - bootstrap - INFO - Write runtime cluster config\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:41,492 - bootstrap - INFO - Resource Config is: {'current_host': 'algo-1', 'current_instance_type': 'ml.m5.xlarge', 'current_group_name': 'homogeneousCluster', 'hosts': ['algo-1'], 'instance_groups': [{'instance_group_name': 'homogeneousCluster', 'instance_type': 'ml.m5.xlarge', 'hosts': ['algo-1']}], 'network_interface_name': 'eth0'}\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:41,499 - bootstrap - INFO - Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:41,500 - bootstrap - INFO - Starting spark process for master node algo-1\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:41,500 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs namenode -format -force\u001b[0m\n",
      "\u001b[34mWARNING: /usr/hadoop-3.0.0/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:41,965 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.178.129\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.0.0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/hadoop-3.0.0/etc/hadoop:/usr/hadoop-3.0.0/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/junit-4.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/aws-java-sdk-bundle-1.11.199.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-aws-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-kms-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okio-1.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-csv-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-el-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-math-2.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/disruptor-3.3.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/findbugs-annotations-1.3.9-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/fst-2.50.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-annotations-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-client-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-common-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop2-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-prefix-tree-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-procedure-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-protocol-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-server-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/htrace-core-3.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jamon-runtime-2.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-compiler-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-runtime-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jcodings-1.0.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/joni-2.1.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-api-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-2.2.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/servlet-api-2.5-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-api-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-registry-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-\u001b[0m\n",
      "\u001b[34mnodemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-router-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.0.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r c25427ceca461ee979d30edd7a4b0f50718e6533; compiled by 'andrew' on 2017-12-08T19:16Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_392\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:41,974 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:41,977 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-c40b1bd3-3e7b-4eab-9c71-8460f990a574\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,509 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,520 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,521 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,524 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,528 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,528 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,528 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,528 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,559 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,569 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,569 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,573 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,576 INFO blockmanagement.BlockManager: The block deletion will start around 2025 Feb 17 05:15:42\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,577 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,577 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,579 INFO util.GSet: 2.0% max memory 3.1 GB = 62.6 MB\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,579 INFO util.GSet: capacity      = 2^23 = 8388608 entries\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,614 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,617 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,617 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,617 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,617 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,617 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,617 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,617 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,617 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,618 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,618 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,618 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,643 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,643 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,643 INFO util.GSet: 1.0% max memory 3.1 GB = 31.3 MB\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,643 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,645 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,645 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,645 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,645 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,650 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,653 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,653 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,653 INFO util.GSet: 0.25% max memory 3.1 GB = 7.8 MB\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,653 INFO util.GSet: capacity      = 2^20 = 1048576 entries\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,690 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,690 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,690 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,693 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,693 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,695 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,695 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,695 INFO util.GSet: 0.029999999329447746% max memory 3.1 GB = 961.8 KB\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,695 INFO util.GSet: capacity      = 2^17 = 131072 entries\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,714 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1844069366-10.0.178.129-1739769342708\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,725 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,732 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,812 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 389 bytes saved in 0 seconds.\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,825 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,829 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.0.178.129\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:42,840 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:44,899 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode, return code 1\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:44,899 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:46,967 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode, return code 1\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:46,967 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:48,061 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager, return code 1\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:48,061 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:50,225 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager, return code 1\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:50,226 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:52,404 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver, return code 1\u001b[0m\n",
      "\u001b[34m2025-02-17 05:15:52,404 - DefaultDataAnalyzer - INFO - Total number of hosts in the cluster: 1\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:02,415 - DefaultDataAnalyzer - INFO - Running command: bin/spark-submit --master yarn --deploy-mode client --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider --conf spark.serializer=org.apache.spark.serializer.KryoSerializer /opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:04,091 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:04,464 INFO Main: Start analyzing with args: --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:04,505 INFO Main: Analytics input path: DataAnalyzerParams(/tmp/spark_job_config.json,yarn)\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:04,516 INFO FileUtil: Read file from path /tmp/spark_job_config.json.\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:05,158 INFO spark.SparkContext: Running Spark version 3.3.0\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:05,184 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:05,185 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:05,185 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:05,186 INFO spark.SparkContext: Submitted application: SageMakerDataAnalyzer\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:05,212 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 3, script: , vendor: , memory -> name: memory, amount: 11507, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:05,226 INFO resource.ResourceProfile: Limiting resource is cpus at 3 tasks per executor\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:05,228 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:05,281 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:05,281 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:05,281 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:05,282 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:05,282 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:05,664 INFO util.Utils: Successfully started service 'sparkDriver' on port 41233.\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:05,699 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:05,736 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:05,759 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:05,759 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:05,791 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:05,813 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-668b930a-234d-4146-a329-eb9ffdf27fc6\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:05,830 INFO memory.MemoryStore: MemoryStore started with capacity 1458.6 MiB\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:05,868 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:05,901 INFO spark.SparkContext: Added JAR file:/opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar at spark://10.0.178.129:41233/jars/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar with timestamp 1739769365151\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:06,386 INFO client.RMProxy: Connecting to ResourceManager at /10.0.178.129:8032\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:07,050 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:07,050 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:07,057 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (15692 MB per container)\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:07,057 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:07,057 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:07,058 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:07,064 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:07,144 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:09,304 INFO yarn.Client: Uploading resource file:/tmp/spark-b1262905-505a-4f5a-93e9-51d7b7b1c9c4/__spark_libs__6350449088609835792.zip -> hdfs://10.0.178.129/user/root/.sparkStaging/application_1739769348817_0001/__spark_libs__6350449088609835792.zip\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:10,423 INFO yarn.Client: Uploading resource file:/tmp/spark-b1262905-505a-4f5a-93e9-51d7b7b1c9c4/__spark_conf__6606452221382836373.zip -> hdfs://10.0.178.129/user/root/.sparkStaging/application_1739769348817_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:10,482 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:10,483 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:10,483 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:10,483 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:10,483 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:10,510 INFO yarn.Client: Submitting application application_1739769348817_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:10,688 INFO impl.YarnClientImpl: Submitted application application_1739769348817_0001\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:11,694 INFO yarn.Client: Application report for application_1739769348817_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:11,698 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: [Mon Feb 17 05:16:11 +0000 2025] Scheduler has assigned a container for AM, waiting for AM container to be launched\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1739769370604\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1739769348817_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:12,704 INFO yarn.Client: Application report for application_1739769348817_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:13,706 INFO yarn.Client: Application report for application_1739769348817_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:14,709 INFO yarn.Client: Application report for application_1739769348817_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:15,713 INFO yarn.Client: Application report for application_1739769348817_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:15,714 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.0.178.129\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1739769370604\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1739769348817_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:15,715 INFO cluster.YarnClientSchedulerBackend: Application application_1739769348817_0001 has started running.\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:15,725 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43511.\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:15,725 INFO netty.NettyBlockTransferService: Server created on 10.0.178.129:43511\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:15,727 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:15,738 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.178.129, 43511, None)\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:15,742 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.178.129:43511 with 1458.6 MiB RAM, BlockManagerId(driver, 10.0.178.129, 43511, None)\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:15,745 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.178.129, 43511, None)\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:15,747 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.178.129, 43511, None)\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:15,866 INFO util.log: Logging initialized @13193ms to org.sparkproject.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:15,944 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1739769348817_0001), /proxy/application_1739769348817_0001\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:17,232 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:20,236 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.178.129:36200) with ID 1,  ResourceProfileId 0\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:20,435 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-1:45117 with 5.8 GiB RAM, BlockManagerId(1, algo-1, 45117, None)\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:36,329 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:36,524 WARN spark.SparkContext: Spark is not running in local mode, therefore the checkpoint directory must not be on the local filesystem. Directory '/tmp' appears to be on the local filesystem.\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:36,562 ERROR helpers.DatasetReader: Could not find any json file under directory /opt/ml/processing/input/baseline_dataset_input. Please verify if the provided dataset path is correct or if data capturing in your Endpoint is turned on.\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:36,574 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:36,636 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:36,637 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:36,642 INFO cluster.YarnClientSchedulerBackend: YARN client scheduler backend Stopped\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:36,662 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:36,702 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:36,706 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:36,726 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:36,745 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:36,799 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:36,801 ERROR Main: Error: Could not find any json file under directory /opt/ml/processing/input/baseline_dataset_input. Please verify if the provided dataset path is correct or if data capturing in your Endpoint is turned on.\u001b[0m\n",
      "\u001b[34mcom.amazonaws.sagemaker.dataanalyzer.exception.CustomerError: Could not find any json file under directory /opt/ml/processing/input/baseline_dataset_input. Please verify if the provided dataset path is correct or if data capturing in your Endpoint is turned on.\u001b[0m\n",
      "\u001b[34m#011at com.amazonaws.sagemaker.dataanalyzer.helpers.DatasetReader.getDataSetPath(DatasetReader.scala:175)\u001b[0m\n",
      "\u001b[34m#011at com.amazonaws.sagemaker.dataanalyzer.helpers.DatasetReader.read(DatasetReader.scala:57)\u001b[0m\n",
      "\u001b[34m#011at com.amazonaws.sagemaker.dataanalyzer.Main$.$anonfun$main$1(Main.scala:55)\u001b[0m\n",
      "\u001b[34m#011at com.amazonaws.sagemaker.dataanalyzer.Main$.$anonfun$main$1$adapted(Main.scala:42)\u001b[0m\n",
      "\u001b[34m#011at com.amazonaws.sagemaker.dataanalyzer.Main$.withSpark(Main.scala:176)\u001b[0m\n",
      "\u001b[34m#011at com.amazonaws.sagemaker.dataanalyzer.Main$.main(Main.scala:42)\u001b[0m\n",
      "\u001b[34m#011at com.amazonaws.sagemaker.dataanalyzer.Main.main(Main.scala)\u001b[0m\n",
      "\u001b[34m#011at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\u001b[0m\n",
      "\u001b[34m#011at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\u001b[0m\n",
      "\u001b[34m#011at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\u001b[0m\n",
      "\u001b[34m#011at java.lang.reflect.Method.invoke(Method.java:498)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:958)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:36,808 INFO Main: Write to file /opt/ml/output/message.\u001b[0m\n",
      "\u001b[34mException in thread \"main\" com.amazonaws.sagemaker.dataanalyzer.exception.CustomerError: Error: Could not find any json file under directory /opt/ml/processing/input/baseline_dataset_input. Please verify if the provided dataset path is correct or if data capturing in your Endpoint is turned on.\u001b[0m\n",
      "\u001b[34m#011at com.amazonaws.sagemaker.dataanalyzer.Main$.main(Main.scala:87)\u001b[0m\n",
      "\u001b[34m#011at com.amazonaws.sagemaker.dataanalyzer.Main.main(Main.scala)\u001b[0m\n",
      "\u001b[34m#011at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\u001b[0m\n",
      "\u001b[34m#011at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\u001b[0m\n",
      "\u001b[34m#011at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\u001b[0m\n",
      "\u001b[34m#011at java.lang.reflect.Method.invoke(Method.java:498)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:958)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:36,861 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:36,864 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-c4b2cec3-1ca7-4100-9244-ae1f459e302e\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:36,877 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-b1262905-505a-4f5a-93e9-51d7b7b1c9c4\u001b[0m\n",
      "\u001b[34m2025-02-17 05:16:36,937 - __main__ - ERROR - Exception performing analysis: Command 'bin/spark-submit --master yarn --deploy-mode client --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider --conf spark.serializer=org.apache.spark.serializer.KryoSerializer /opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar --analytics_input /tmp/spark_job_config.json' returned non-zero exit status 1.\u001b[0m\n",
      "\n",
      "Error running baseline suggestion job: Error for Processing job baseline-suggestion-job-2025-02-17-05-13-52-651: Failed. Reason: AlgorithmError: Error: Could not find any json file under directory /opt/ml/processing/input/baseline_dataset_input. Please verify if the provided dataset path is correct or if data capturing in your Endpoint is turned on., exit code: 255\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Processing job baseline-suggestion-job-2025-02-17-05-13-52-651: Failed. Reason: AlgorithmError: Error: Could not find any json file under directory /opt/ml/processing/input/baseline_dataset_input. Please verify if the provided dataset path is correct or if data capturing in your Endpoint is turned on., exit code: 255",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning baseline suggestion job...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mmodel_monitor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuggest_baseline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbaseline_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbaseline_dataset_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Path containing JSONL files\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDatasetFormat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Ensure input matches JSON\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_s3_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbaseline_results_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# Where baseline results will be stored\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m                               \u001b[49m\u001b[38;5;66;43;03m# Wait for the job to complete\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBaseline job completed. Results saved at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbaseline_results_uri\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/model_monitor/model_monitoring.py:1922\u001b[0m, in \u001b[0;36mDefaultModelMonitor.suggest_baseline\u001b[0;34m(self, baseline_dataset, dataset_format, record_preprocessor_script, post_analytics_processor_script, output_s3_uri, wait, logs, job_name, monitoring_config_override)\u001b[0m\n\u001b[1;32m   1910\u001b[0m baseline_job_inputs_with_nones \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1911\u001b[0m     normalized_baseline_dataset_input,\n\u001b[1;32m   1912\u001b[0m     normalized_record_preprocessor_script_input,\n\u001b[1;32m   1913\u001b[0m     normalized_post_processor_script_input,\n\u001b[1;32m   1914\u001b[0m ]\n\u001b[1;32m   1916\u001b[0m baseline_job_inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1917\u001b[0m     baseline_job_input\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m baseline_job_input \u001b[38;5;129;01min\u001b[39;00m baseline_job_inputs_with_nones\n\u001b[1;32m   1919\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m baseline_job_input \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1920\u001b[0m ]\n\u001b[0;32m-> 1922\u001b[0m \u001b[43mbaselining_processor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1923\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbaseline_job_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1924\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mnormalized_baseline_output\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43marguments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marguments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjob_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatest_baselining_job_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1929\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1931\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_baselining_job \u001b[38;5;241m=\u001b[39m BaseliningJob\u001b[38;5;241m.\u001b[39mfrom_processing_job(\n\u001b[1;32m   1932\u001b[0m     processing_job\u001b[38;5;241m=\u001b[39mbaselining_processor\u001b[38;5;241m.\u001b[39mlatest_job\n\u001b[1;32m   1933\u001b[0m )\n\u001b[1;32m   1934\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbaselining_jobs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_baselining_job)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/workflow/pipeline_context.py:346\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/processing.py:277\u001b[0m, in \u001b[0;36mProcessor.run\u001b[0;34m(self, inputs, outputs, arguments, wait, logs, job_name, experiment_config, kms_key)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjobs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_job)\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatest_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/processing.py:1113\u001b[0m, in \u001b[0;36mProcessingJob.wait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Waits for the processing job to complete.\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \n\u001b[1;32m   1108\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;124;03m    logs (bool): Whether to show the logs produced by the job (default: True).\u001b[39;00m\n\u001b[1;32m   1110\u001b[0m \n\u001b[1;32m   1111\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logs:\n\u001b[0;32m-> 1113\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogs_for_processing_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39mwait_for_processing_job(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_name)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/session.py:5929\u001b[0m, in \u001b[0;36mSession.logs_for_processing_job\u001b[0;34m(self, job_name, wait, poll)\u001b[0m\n\u001b[1;32m   5926\u001b[0m             state \u001b[38;5;241m=\u001b[39m LogState\u001b[38;5;241m.\u001b[39mJOB_COMPLETE\n\u001b[1;32m   5928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 5929\u001b[0m     \u001b[43m_check_job_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mProcessingJobStatus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5930\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dot:\n\u001b[1;32m   5931\u001b[0m         \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/session.py:8508\u001b[0m, in \u001b[0;36m_check_job_status\u001b[0;34m(job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   8502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCapacityError\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(reason):\n\u001b[1;32m   8503\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCapacityError(\n\u001b[1;32m   8504\u001b[0m         message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   8505\u001b[0m         allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   8506\u001b[0m         actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   8507\u001b[0m     )\n\u001b[0;32m-> 8508\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mUnexpectedStatusException(\n\u001b[1;32m   8509\u001b[0m     message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   8510\u001b[0m     allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   8511\u001b[0m     actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   8512\u001b[0m )\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Processing job baseline-suggestion-job-2025-02-17-05-13-52-651: Failed. Reason: AlgorithmError: Error: Could not find any json file under directory /opt/ml/processing/input/baseline_dataset_input. Please verify if the provided dataset path is correct or if data capturing in your Endpoint is turned on., exit code: 255"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"Running baseline suggestion job...\")\n",
    "\n",
    "    model_monitor.suggest_baseline(\n",
    "        baseline_dataset=baseline_dataset_path,  # Path containing JSONL files\n",
    "        dataset_format=DatasetFormat.json(),    # Ensure input matches JSON\n",
    "        output_s3_uri=baseline_results_uri,     # Where baseline results will be stored\n",
    "        wait=True                               # Wait for the job to complete\n",
    "    )\n",
    "\n",
    "    print(f\"Baseline job completed. Results saved at: {baseline_results_uri}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error running baseline suggestion job: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c817d72d-6279-41b3-8574-2a25ab390331",
   "metadata": {},
   "source": [
    "### Verify Baseline Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94691c51-9338-4a5c-a782-1bd3ab184d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine results of baseline job\n",
    "baseline_statistics_path = f\"{baseline_results_uri}statistics.json\"\n",
    "baseline_constraints_path = f\"{baseline_results_uri}constraints.json\"\n",
    "\n",
    "print(\"Baseline statistics stored at:\", baseline_statistics_path)\n",
    "print(\"Baseline constraints stored at:\", baseline_constraints_path)\n",
    "\n",
    "# Download and inspect baseline results\n",
    "s3_client.download_file(bucket, \"flight-delay-baseline-results/statistics.json\", \"statistics.json\")\n",
    "s3_client.download_file(bucket, \"flight-delay-baseline-results/constraints.json\", \"constraints.json\")\n",
    "\n",
    "import json\n",
    "with open(\"statistics.json\", \"r\") as stats_file:\n",
    "    baseline_statistics = json.load(stats_file)\n",
    "    print(\"Sample Baseline Statistics:\", json.dumps(baseline_statistics, indent=2))\n",
    "\n",
    "with open(\"constraints.json\", \"r\") as constraints_file:\n",
    "    baseline_constraints = json.load(constraints_file)\n",
    "    print(\"Sample Baseline Constraints:\", json.dumps(baseline_constraints, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6eed3d8-85ad-42e6-9975-15b288b0f065",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7ce8a4-9816-471a-b606-f2b78e66c37a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5444d09-c689-4b26-9b97-80fab27617fa",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">TODO - make alert in cloudwatch to trigger</span>\n",
    "### below - saves random data to endpoint to trigger alert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a0622ad-3269-4246-aaba-b33f6cc0804f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "Baseline Dataset Path: s3://{bucket}/data_capture/flight-delay-xgboost-endpoint-single-request/AllTraffic/\n",
      "Baseline Results Path: s3://sagemaker-us-east-2-450772039932/baseline_results/\n",
      "Monitoring Results Path: s3://sagemaker-us-east-2-450772039932/monitoring_results/\n",
      "Monitoring schedule created for endpoint: flight-delay-xgboost-endpoint-single-request\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.model_monitor import DefaultModelMonitor, DatasetFormat, CronExpressionGenerator\n",
    "\n",
    "# Initialize Session and Define Bucket/Prefix\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "prefix = \"flight-delay-prediction-xgboost\"\n",
    "\n",
    "# Monitoring and baseline results paths\n",
    "baseline_results_uri = f\"s3://{bucket}/baseline_results/\"\n",
    "monitoring_results_uri = f\"s3://{bucket}/monitoring_results/\"\n",
    "\n",
    "# Ensure Data Capture Paths (jsonl paths from captured traffic)\n",
    "baseline_dataset_path = \"s3://{bucket}/data_capture/flight-delay-xgboost-endpoint-single-request/AllTraffic/\"\n",
    "\n",
    "print(\"Baseline Dataset Path:\", baseline_dataset_path)\n",
    "print(\"Baseline Results Path:\", baseline_results_uri)\n",
    "print(\"Monitoring Results Path:\", monitoring_results_uri)\n",
    "\n",
    "# Configure the model monitor\n",
    "model_monitor = DefaultModelMonitor(\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    max_runtime_in_seconds=3600,\n",
    "    sagemaker_session=session,\n",
    ")\n",
    "\n",
    "# Schedule Data Monitoring\n",
    "monitor_schedule_name = \"FlightDelayMonitor\"\n",
    "model_monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=monitor_schedule_name,\n",
    "    endpoint_input=\"flight-delay-xgboost-endpoint-single-request\",  # Endpoint being monitored\n",
    "    output_s3_uri=monitoring_results_uri,\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "    enable_cloudwatch_metrics=True,\n",
    ")\n",
    "\n",
    "print(f\"Monitoring schedule created for endpoint: flight-delay-xgboost-endpoint-single-request\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e12b019d-b9ba-4da6-9391-b9f295e26bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fake data generated and saved to fake_flight_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Feature columns (ensure these match training data)\n",
    "columns = [\n",
    "    \"year\", \"month\", \"day\", \"carrier\", \"airport\", \n",
    "    \"arr_flights\", \"arr_del15\", \"carrier_ct\", \"weather_ct\", \n",
    "    \"nas_ct\", \"security_ct\", \"late_aircraft_ct\", \"arr_cancelled\", \n",
    "    \"arr_diverted\", \"arr_delay\", \"carrier_delay\", \"weather_delay\", \n",
    "    \"nas_delay\", \"security_delay\", \"late_aircraft_delay\", \"delay_rate\"\n",
    "]\n",
    "\n",
    "# Generate random fake data\n",
    "def generate_fake_data(num_rows=100):\n",
    "    fake_data = []\n",
    "    for _ in range(num_rows):\n",
    "        fake_row = [\n",
    "            random.randint(2000, 2023),  # year\n",
    "            random.randint(1, 12),  # month\n",
    "            random.randint(1, 28),  # day\n",
    "            random.choice([\"AA\", \"DL\", \"UA\", \"SW\", \"AS\", \"EV\"]),  # carrier\n",
    "            random.choice([\"JFK\", \"LAX\", \"ORD\", \"ATL\", \"DFW\", \"DEN\"]),  # airport\n",
    "            random.randint(50, 1000),  # arr_flights\n",
    "            random.randint(0, 200),  # arr_del15\n",
    "            random.randint(0, 50),  # carrier_ct\n",
    "            random.randint(0, 50),  # weather_ct\n",
    "            random.randint(0, 50),  # nas_ct\n",
    "            random.randint(0, 50),  # security_ct\n",
    "            random.randint(0, 50),  # late_aircraft_ct\n",
    "            random.randint(0, 10),  # arr_cancelled\n",
    "            random.randint(0, 10),  # arr_diverted\n",
    "            random.uniform(0, 300),  # arr_delay\n",
    "            random.uniform(0, 100),  # carrier_delay\n",
    "            random.uniform(0, 100),  # weather_delay\n",
    "            random.uniform(0, 100),  # nas_delay\n",
    "            random.uniform(0, 100),  # security_delay\n",
    "            random.uniform(0, 100),  # late_aircraft_delay\n",
    "            random.uniform(1.0, 2.0),  # delay_rate\n",
    "        ]\n",
    "        fake_data.append(fake_row)\n",
    "    return pd.DataFrame(fake_data, columns=columns)\n",
    "\n",
    "# Write fake data to CSV\n",
    "fake_data = generate_fake_data(500)  # Generate 500 records\n",
    "fake_data_file = \"fake_flight_data.csv\"\n",
    "fake_data.to_csv(fake_data_file, index=False, header=False)\n",
    "print(f\"Fake data generated and saved to {fake_data_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a2c81a-376f-4f25-bfe1-73934c8cf0ca",
   "metadata": {},
   "source": [
    "### Clean up script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b6e8d75-f265-43cc-9bbe-f33541c07d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing monitoring schedules for endpoint: flight-delay-xgboost-endpoint-single-request\n",
      "Deleting monitoring schedule: FlightDelayMonitor\n",
      "All monitoring schedules deleted.\n",
      "Deleting endpoint: flight-delay-xgboost-endpoint-single-request\n",
      "Endpoint 'flight-delay-xgboost-endpoint-single-request' deleted.\n",
      "Deleting endpoint configuration: flight-delay-xgboost-endpoint-single-request-config\n",
      "Error while deleting endpoint configuration: An error occurred (ValidationException) when calling the DeleteEndpointConfig operation: Could not find endpoint configuration \"flight-delay-xgboost-endpoint-single-request-config\".\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ValidationException) when calling the DeleteEndpointConfig operation: Could not find endpoint configuration \"flight-delay-xgboost-endpoint-single-request-config\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeleting endpoint configuration: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mendpoint_config_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 43\u001b[0m     \u001b[43msagemaker_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelete_endpoint_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEndpointConfigName\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint_config_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEndpoint configuration \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mendpoint_config_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m deleted.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m sagemaker_client\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mResourceNotFound \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/client.py:569\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    566\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m     )\n\u001b[1;32m    568\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 569\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/client.py:1023\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1020\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1021\u001b[0m     )\n\u001b[1;32m   1022\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1023\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ValidationException) when calling the DeleteEndpointConfig operation: Could not find endpoint configuration \"flight-delay-xgboost-endpoint-single-request-config\"."
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from time import sleep\n",
    "\n",
    "# Initialize SageMaker client\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "# Define resources to clean up\n",
    "endpoint_name = \"flight-delay-xgboost-endpoint-single-request\"\n",
    "monitoring_schedule_name = \"ModelMonitorForXGBoost\"  # Adjust if the schedule name is different\n",
    "endpoint_config_name = f\"{endpoint_name}-config\"\n",
    "\n",
    "# Step 1: Delete Monitoring Schedules\n",
    "try:\n",
    "    # List all monitoring schedules associated with the endpoint\n",
    "    print(f\"Listing monitoring schedules for endpoint: {endpoint_name}\")\n",
    "    monitoring_schedules = sagemaker_client.list_monitoring_schedules()['MonitoringScheduleSummaries']\n",
    "    for schedule in monitoring_schedules:\n",
    "        if schedule['EndpointName'] == endpoint_name:\n",
    "            print(f\"Deleting monitoring schedule: {schedule['MonitoringScheduleName']}\")\n",
    "            sagemaker_client.delete_monitoring_schedule(MonitoringScheduleName=schedule['MonitoringScheduleName'])\n",
    "            sleep(10)  # Allow time for the monitoring schedule deletion process\n",
    "    print(\"All monitoring schedules deleted.\")\n",
    "except sagemaker_client.exceptions.ResourceNotFound as e:\n",
    "    print(\"Monitoring schedule not found. Skipping deletion.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error while deleting monitoring schedules: {e}\")\n",
    "    raise\n",
    "\n",
    "# Step 2: Delete Endpoint\n",
    "try:\n",
    "    print(f\"Deleting endpoint: {endpoint_name}\")\n",
    "    sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "    print(f\"Endpoint '{endpoint_name}' deleted.\")\n",
    "except sagemaker_client.exceptions.ResourceNotFound as e:\n",
    "    print(f\"Endpoint '{endpoint_name}' not found. Skipping deletion.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error while deleting endpoint: {e}\")\n",
    "    raise\n",
    "\n",
    "# Step 3: Delete Endpoint Configuration\n",
    "try:\n",
    "    print(f\"Deleting endpoint configuration: {endpoint_config_name}\")\n",
    "    sagemaker_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "    print(f\"Endpoint configuration '{endpoint_config_name}' deleted.\")\n",
    "except sagemaker_client.exceptions.ResourceNotFound as e:\n",
    "    print(f\"Endpoint configuration '{endpoint_config_name}' not found. Skipping deletion.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error while deleting endpoint configuration: {e}\")\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
