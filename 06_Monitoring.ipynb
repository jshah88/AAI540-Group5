{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0891c67-ad7d-4b58-a211-349b1eb5e462",
   "metadata": {},
   "source": [
    "# Feature / Data Drift Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7eb7b1e-685b-4060-a3fb-f82460c406f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored variables and their in-db values:\n",
      "account_id                                -> '607916531205'\n",
      "baseline_model_logistic_path              -> 'baseline_model_logistic.pkl'\n",
      "baseline_model_path                       -> 'baseline_model.pkl'\n",
      "create_base_csv_athena_db                 -> True\n",
      "create_base_csv_athena_table              -> True\n",
      "database_name                             -> 'db_airline_delay_cause'\n",
      "dev_feature_group_name                    -> 'airline_delay_features_dev'\n",
      "dev_feature_store_table                   -> 'airline_delay_features_dev_1740273029'\n",
      "dev_s3_path                               -> 's3://sagemaker-us-east-1-607916531205/data/develo\n",
      "dev_s3_uri                                -> 's3://sagemaker-us-east-1-607916531205/feature-sto\n",
      "dev_table_name                            -> 'development_data'\n",
      "endpoint_name_batch_transform             -> 'flight-delay-xgboost-endpoint-with-batch-transfor\n",
      "endpoint_name_single_request              -> 'flight-delay-xgboost-endpoint-single-request'\n",
      "packages_installed                        -> True\n",
      "prod_feature_group_name                   -> 'airline_delay_features_prod'\n",
      "prod_feature_store_table                  -> 'airline_delay_features_prod_1740273120'\n",
      "prod_s3_path                              -> 's3://sagemaker-us-east-1-607916531205/data/produc\n",
      "prod_s3_uri                               -> 's3://sagemaker-us-east-1-607916531205/feature-sto\n",
      "prod_table_name                           -> 'production_data'\n",
      "raw_table_name                            -> 'airline_delay_cause_csv_raw'\n",
      "region                                    -> 'us-east-1'\n",
      "role                                      -> 'arn:aws:iam::607916531205:role/LabRole'\n",
      "s3_csv_private_path                       -> 's3://sagemaker-us-east-1-607916531205/airline-del\n",
      "s3_staging_dir                            -> 's3://sagemaker-us-east-1-607916531205/athena/stag\n",
      "setup_s3_bucket_passed                    -> True\n"
     ]
    }
   ],
   "source": [
    "%store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c1f5cf-db15-4b28-907e-07ddcd8223e7",
   "metadata": {},
   "source": [
    "### Data Capture Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58939eb3-761c-4372-bec1-6d024b146ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "✅ JSON files available in data capture S3 path:\n",
      "- data_capture/flight-delay-xgboost-endpoint-single-request/AllTraffic/2025/02/23/01/55-07-422-b3e164c6-4691-462b-ab06-b3aa78bfaf9c.jsonl\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "# ✅ Retrieve stored variables\n",
    "%store -r endpoint_name_single_request\n",
    "%store -r s3_staging_dir\n",
    "%store -r role  \n",
    "\n",
    "# ✅ Initialize AWS session\n",
    "session = boto3.session.Session()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# ✅ Reinitialize `bucket`\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "# ✅ Initialize S3 client\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "# ✅ Ensure `endpoint_name_single_request` exists before using it\n",
    "if 'endpoint_name_single_request' not in locals() or not endpoint_name_single_request:\n",
    "    print(\"⚠️ `endpoint_name_single_request` is not set. Make sure 05-train-and-deploy.ipynb was run successfully.\")\n",
    "else:\n",
    "    # ✅ Define S3 prefix for captured data dynamically\n",
    "    data_capture_prefix = f\"data_capture/{endpoint_name_single_request}/AllTraffic/\"\n",
    "\n",
    "    # ✅ List objects in the S3 data capture folder\n",
    "    capture_files = s3_client.list_objects_v2(Bucket=bucket, Prefix=data_capture_prefix)\n",
    "\n",
    "    if \"Contents\" in capture_files:\n",
    "        print(\"✅ JSON files available in data capture S3 path:\")\n",
    "        for obj in capture_files[\"Contents\"]:\n",
    "            print(f\"- {obj['Key']}\")\n",
    "    else:\n",
    "        print(\"⚠️ No JSON files found. Ensure the endpoint is receiving inference requests.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cbcef7-8a50-4aaf-a454-160ee97ed091",
   "metadata": {},
   "source": [
    "## Obtain Samples from Dev Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eca2e432-425c-4504-a6a2-a29cf0810d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_139/1325788494.py:18: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  validation_data = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     event_time  year  month carrier airport  arr_flights  arr_del15  \\\n",
      "0  1.740273e+09  2015      5      OO     SMX           63         22   \n",
      "1  1.740273e+09  2013      4      DL     MYR           29          5   \n",
      "2  1.740273e+09  2015      5      OO     FWA          341         56   \n",
      "3  1.740273e+09  2008      6      OO     CHS           10          6   \n",
      "4  1.740273e+09  2008      6      OO     CPR          167         21   \n",
      "\n",
      "   carrier_ct  weather_ct  nas_ct  ...  weather_delay  nas_delay  \\\n",
      "0           2           0       1  ...              0         32   \n",
      "1           3           0       0  ...             26         57   \n",
      "2          20           2       8  ...            195        336   \n",
      "3           1           0       2  ...            114        142   \n",
      "4           6           0       6  ...              0        282   \n",
      "\n",
      "   security_delay  late_aircraft_delay  delay_rate  on_time  record_id  \\\n",
      "0               0                  998          34        0     123259   \n",
      "1               0                   18          17        0      97268   \n",
      "2               0                 1263          16        0     123275   \n",
      "3               0                  321          60        0      45022   \n",
      "4               0                  971          12        0      45030   \n",
      "\n",
      "               write_time  api_invocation_time  is_deleted  \n",
      "0 2025-02-23 01:23:26.210  2025-02-23 01:18:25       False  \n",
      "1 2025-02-23 01:23:26.210  2025-02-23 01:18:26       False  \n",
      "2 2025-02-23 01:23:26.210  2025-02-23 01:18:26       False  \n",
      "3 2025-02-23 01:23:26.210  2025-02-23 01:18:26       False  \n",
      "4 2025-02-23 01:23:26.210  2025-02-23 01:18:26       False  \n",
      "\n",
      "[5 rows x 26 columns]\n",
      "Validation data shape: (1000, 26)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "from pyathena import connect\n",
    "\n",
    "# ✅ Retrieve stored variables\n",
    "%store -r dev_feature_store_table\n",
    "%store -r s3_staging_dir\n",
    "%store -r region\n",
    "\n",
    "# ✅ Define Athena query to fetch validation data from the feature store\n",
    "query = f\"\"\"\n",
    "SELECT * FROM \"sagemaker_featurestore\".\"{dev_feature_store_table}\"\n",
    "LIMIT 1000  -- Adjust the limit if needed\n",
    "\"\"\"\n",
    "\n",
    "# ✅ Connect to Athena and execute query\n",
    "conn = connect(s3_staging_dir=s3_staging_dir, region_name=region)\n",
    "validation_data = pd.read_sql(query, conn)\n",
    "\n",
    "# ✅ Display the first few rows to confirm correct loading\n",
    "print(validation_data.head())\n",
    "print(f\"Validation data shape: {validation_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62f23b84-0edc-4c8e-be0e-dc9d5776c3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Prepared 50 rows with label-encoded categorical features for inference.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ✅ Define subset size\n",
    "subset_size = 50  # Number of rows to send to the endpoint\n",
    "\n",
    "# ✅ Retrieve categorical columns that need encoding\n",
    "categorical_columns = [\"carrier\", \"airport\"]\n",
    "\n",
    "# ✅ Create label encoders and fit on the validation data\n",
    "label_encoders = {}\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    validation_data[col] = le.fit_transform(validation_data[col])  # Apply encoding\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# ✅ Define columns to exclude (target + metadata)\n",
    "columns_to_exclude = [\n",
    "    \"delay_rate\", \"on_time\",  # Target variables\n",
    "    \"record_id\", \"write_time\", \"api_invocation_time\", \"is_deleted\"  # Metadata columns\n",
    "]\n",
    "\n",
    "# ✅ Select only valid feature columns\n",
    "feature_columns = [col for col in validation_data.columns if col not in columns_to_exclude]\n",
    "\n",
    "# ✅ Select subset of validation data with correct features\n",
    "subset_data = validation_data[feature_columns].iloc[:subset_size]\n",
    "\n",
    "# ✅ Convert to CSV-like strings for SageMaker endpoint\n",
    "subset_data_str_list = subset_data.apply(lambda row: \",\".join(row.astype(str)), axis=1).tolist()\n",
    "\n",
    "print(f\"✅ Prepared {len(subset_data_str_list)} rows with label-encoded categorical features for inference.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e7d944-abc4-4228-96a0-ef74e46ef7e9",
   "metadata": {},
   "source": [
    "## Create predictor endpoint and send data to endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f639d32-0a9e-4b4f-98cc-47b10dceed5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🟢 Input: 1740273202.9792993,2015.0,5.0,6.0,217.0,63.0,22.0,2.0,0.0,1.0,0.0,18.0,0.0,0.0,1269.0,239.0,0.0,32.0,0.0,998.0\n",
      "🔵 Prediction: {'predictions': [{'score': 1.0346633644076064e-05}]}\n",
      "🟢 Input: 1740273202.9792993,2013.0,4.0,3.0,163.0,29.0,5.0,3.0,0.0,0.0,0.0,0.0,0.0,0.0,269.0,168.0,26.0,57.0,0.0,18.0\n",
      "🔵 Prediction: {'predictions': [{'score': 1.1478582564450335e-05}]}\n",
      "🟢 Input: 1740273202.9792993,2015.0,5.0,6.0,87.0,341.0,56.0,20.0,2.0,8.0,0.0,24.0,3.0,0.0,3001.0,1207.0,195.0,336.0,0.0,1263.0\n",
      "🔵 Prediction: {'predictions': [{'score': 4.337779500929173e-06}]}\n",
      "🟢 Input: 1740273202.9792993,2008.0,6.0,6.0,46.0,10.0,6.0,1.0,0.0,2.0,0.0,2.0,1.0,0.0,617.0,40.0,114.0,142.0,0.0,321.0\n",
      "🔵 Prediction: {'predictions': [{'score': 7.515838206018088e-06}]}\n",
      "🟢 Input: 1740273202.9792993,2008.0,6.0,6.0,55.0,167.0,21.0,6.0,0.0,6.0,0.0,8.0,1.0,0.0,1686.0,433.0,0.0,282.0,0.0,971.0\n",
      "🔵 Prediction: {'predictions': [{'score': 4.337779500929173e-06}]}\n",
      "✅ Finished sending 50 rows to endpoint.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "# ✅ Retrieve stored endpoint name\n",
    "%store -r endpoint_name_single_request\n",
    "\n",
    "# ✅ Check if endpoint_name exists before proceeding\n",
    "if not endpoint_name_single_request:\n",
    "    print(\"⚠️ Endpoint name is missing. Ensure 05-train-and-deploy.ipynb was run successfully.\")\n",
    "else:\n",
    "    # ✅ Initialize the endpoint predictor dynamically\n",
    "    predictor = Predictor(\n",
    "        endpoint_name=endpoint_name_single_request,  # Now using stored variable\n",
    "        sagemaker_session=sagemaker.Session(),\n",
    "        serializer=CSVSerializer(),\n",
    "        deserializer=JSONDeserializer(),\n",
    "    )\n",
    "\n",
    "    # ✅ Send each row to the endpoint (with error handling)\n",
    "    responses = []\n",
    "\n",
    "    if not subset_data_str_list:\n",
    "        print(\"⚠️ No data available to send for inference. Ensure validation data was properly loaded.\")\n",
    "    else:\n",
    "        for i, row_str in enumerate(subset_data_str_list):\n",
    "            try:\n",
    "                response = predictor.predict(row_str)  # Send to the endpoint\n",
    "                responses.append(response)\n",
    "                \n",
    "                # ✅ Print only the first 5 rows to avoid flooding logs\n",
    "                if i < 5:\n",
    "                    print(f\"🟢 Input: {row_str}\")\n",
    "                    print(f\"🔵 Prediction: {response}\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error making prediction for row {i}: {str(e)}\")\n",
    "\n",
    "        print(f\"✅ Finished sending {len(responses)} rows to endpoint.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064a0d1a-c5b7-46ec-970c-620f0901d993",
   "metadata": {},
   "source": [
    "## Validate that JSONL files were captured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c2ab1ed-3539-4b03-ab1d-41bdcf524572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Files found in S3 data capture path (data_capture/flight-delay-xgboost-endpoint-single-request/AllTraffic/):\n",
      "📄 data_capture/flight-delay-xgboost-endpoint-single-request/AllTraffic/2025/02/23/01/55-07-422-b3e164c6-4691-462b-ab06-b3aa78bfaf9c.jsonl\n",
      "📄 data_capture/flight-delay-xgboost-endpoint-single-request/AllTraffic/2025/02/23/04/58-47-981-b165b78d-2f81-4bd3-b5ae-23a79697e023.jsonl\n",
      "✅ All files listed.\n"
     ]
    }
   ],
   "source": [
    "# ✅ Use already initialized session, bucket, and S3 client\n",
    "data_capture_prefix = f\"data_capture/{endpoint_name_single_request}/AllTraffic/\"\n",
    "\n",
    "# ✅ List files in S3 data capture path\n",
    "response = s3_client.list_objects_v2(Bucket=bucket, Prefix=data_capture_prefix)\n",
    "\n",
    "# ✅ Check if \"Contents\" exists in response before accessing it\n",
    "if \"Contents\" in response and response[\"Contents\"]:\n",
    "    print(f\"✅ Files found in S3 data capture path ({data_capture_prefix}):\")\n",
    "\n",
    "    # ✅ Print only the first 5 files to avoid excessive output\n",
    "    for obj in response[\"Contents\"][:5]:  \n",
    "        print(f\"📄 {obj['Key']}\")\n",
    "\n",
    "    print(f\"🟢 ...and {len(response['Contents']) - 5} more files\" if len(response[\"Contents\"]) > 5 else \"✅ All files listed.\")\n",
    "else:\n",
    "    print(f\"⚠️ No files found in S3 data capture path: {data_capture_prefix}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd88afee-152d-4f34-bce8-c470c2a734c1",
   "metadata": {},
   "source": [
    "#### You should now be able to see your data in cloudwatch\n",
    "- Go to Cloudwatch -> Logs -> Log Insights\n",
    "- search `/aws/sagemaker/Endpoints/flight-delay-xgboost-endpoint-single-request`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e5d4d9-2a56-4854-88dc-817e6f4b92d2",
   "metadata": {},
   "source": [
    "## Initialize Baseline Monitoring Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3226806c-7d6b-4687-81fe-e31af556b842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Baseline dataset path: s3://sagemaker-us-east-1-607916531205/data_capture/flight-delay-xgboost-endpoint-single-request/AllTraffic/\n",
      "📂 Baseline results URI: s3://sagemaker-us-east-1-607916531205/baseline_results/\n",
      "Stored 'baseline_results_uri' (str)\n",
      "Stored 'baseline_dataset_path' (str)\n",
      "✅ Stored baseline paths in %store.\n",
      "✅ Model monitor initialized. (Not stored in %store, will be re-created when needed)\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.model_monitor import DefaultModelMonitor, DatasetFormat\n",
    "\n",
    "# ✅ Define Required Variables\n",
    "baseline_results_uri = f\"s3://{bucket}/baseline_results/\"  # Standardized path\n",
    "baseline_dataset_path = f\"s3://{bucket}/data_capture/{endpoint_name_single_request}/AllTraffic/\"\n",
    "\n",
    "print(f\"📂 Baseline dataset path: {baseline_dataset_path}\")\n",
    "print(f\"📂 Baseline results URI: {baseline_results_uri}\")\n",
    "\n",
    "# ✅ Store paths for later use\n",
    "%store baseline_results_uri\n",
    "%store baseline_dataset_path\n",
    "print(\"✅ Stored baseline paths in %store.\")\n",
    "\n",
    "# ✅ Initialize the model monitor (but do NOT store it)\n",
    "model_monitor = DefaultModelMonitor(\n",
    "    role=role,  # ✅ Fixed: Use correct role variable from %store\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    max_runtime_in_seconds=3600,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "print(\"✅ Model monitor initialized. (Not stored in %store, will be re-created when needed)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a1f213-b7be-4481-8d36-be431e3d919e",
   "metadata": {},
   "source": [
    "## Run baseline suggestion job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4bac8ee0-9516-4296-ade9-2e7941b4c7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished sending 50 additional rows. Wait 2-3 minutes before checking data capture in S3.\n"
     ]
    }
   ],
   "source": [
    "# ✅ Send another 50 inference requests\n",
    "responses = []\n",
    "for row_str in subset_data_str_list:\n",
    "    response = predictor.predict(row_str)\n",
    "    responses.append(response)\n",
    "\n",
    "print(\"✅ Finished sending 50 additional rows. Wait 2-3 minutes before checking data capture in S3.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00cebcd4-cebe-401c-9a44-24eb999c1504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found 4 JSONL files in S3:\n",
      "📄 data_capture/flight-delay-xgboost-endpoint-single-request/AllTraffic/2025/02/23/01/55-07-422-b3e164c6-4691-462b-ab06-b3aa78bfaf9c.jsonl\n",
      "📄 data_capture/flight-delay-xgboost-endpoint-single-request/AllTraffic/2025/02/23/04/58-47-981-b165b78d-2f81-4bd3-b5ae-23a79697e023.jsonl\n",
      "📄 data_capture/flight-delay-xgboost-endpoint-single-request/AllTraffic/2025/02/23/05/17-56-062-97d8dfac-98ad-422f-b0b8-e698c22ff525.jsonl\n",
      "📄 data_capture/flight-delay-xgboost-endpoint-with-batch-transform/AllTraffic/2025/02/23/02/06-28-282-7ba30264-befc-49d0-80af-7061a2521244.jsonl\n"
     ]
    }
   ],
   "source": [
    "response = s3_client.list_objects_v2(Bucket=bucket, Prefix=\"data_capture/\")\n",
    "if \"Contents\" in response:\n",
    "    print(f\"✅ Found {len(response['Contents'])} JSONL files in S3:\")\n",
    "    for obj in response[\"Contents\"][:5]:  # Show first 5 files\n",
    "        print(f\"📄 {obj['Key']}\")\n",
    "else:\n",
    "    print(\"⚠️ No new JSONL files found yet. Wait a little longer or resend inferences.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "120e2a3a-3cd8-4869-aec1-e29787b46438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using latest JSONL file: data_capture/flight-delay-xgboost-endpoint-single-request/AllTraffic/2025/02/23/05/17-56-062-97d8dfac-98ad-422f-b0b8-e698c22ff525.jsonl\n",
      "✅ Processed 50 records and saved as CSV.\n",
      "✅ Uploaded fixed CSV file to s3://sagemaker-us-east-1-607916531205/data_capture/flight-delay-xgboost-endpoint-single-request/AllTraffic/fixed_baseline.csv.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import boto3\n",
    "\n",
    "# ✅ Find the latest JSONL file dynamically\n",
    "response = s3_client.list_objects_v2(Bucket=bucket, Prefix=\"data_capture/flight-delay-xgboost-endpoint-single-request/AllTraffic/\")\n",
    "\n",
    "if \"Contents\" not in response or not response[\"Contents\"]:\n",
    "    raise FileNotFoundError(\"⚠️ No JSONL files found. Ensure inference data is being captured.\")\n",
    "\n",
    "# ✅ Get the latest file by sorting timestamps\n",
    "latest_file = sorted(response[\"Contents\"], key=lambda x: x[\"LastModified\"], reverse=True)[0][\"Key\"]\n",
    "\n",
    "print(f\"✅ Using latest JSONL file: {latest_file}\")\n",
    "\n",
    "# ✅ Download the latest JSONL file\n",
    "response = s3_client.get_object(Bucket=bucket, Key=latest_file)\n",
    "file_content = response[\"Body\"].read().decode(\"utf-8\").splitlines()\n",
    "\n",
    "# ✅ Extract only the input feature values from each entry\n",
    "feature_data = []\n",
    "for line in file_content:\n",
    "    entry = json.loads(line)  # Convert JSON string to dict\n",
    "    if \"captureData\" in entry and \"endpointInput\" in entry[\"captureData\"]:\n",
    "        raw_features = entry[\"captureData\"][\"endpointInput\"][\"data\"]\n",
    "        feature_data.append(raw_features.split(\",\"))  # Convert CSV-like string to list\n",
    "\n",
    "# ✅ Convert extracted data to a DataFrame\n",
    "df = pd.DataFrame(feature_data)\n",
    "\n",
    "# ✅ Save locally as CSV\n",
    "fixed_csv_path = \"fixed_baseline.csv\"\n",
    "df.to_csv(fixed_csv_path, index=False, header=False)\n",
    "\n",
    "print(f\"✅ Processed {len(df)} records and saved as CSV.\")\n",
    "\n",
    "# ✅ Upload the processed CSV to S3\n",
    "fixed_s3_path = f\"{baseline_dataset_path}fixed_baseline.csv\"\n",
    "s3_client.upload_file(fixed_csv_path, bucket, fixed_s3_path.replace(f\"s3://{bucket}/\", \"\"))\n",
    "\n",
    "print(f\"✅ Uploaded fixed CSV file to {fixed_s3_path}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "302efffe-b973-49e5-8a11-da1913a4846c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name baseline-suggestion-job-2025-02-23-05-25-05-194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Checking if processed baseline CSV exists in S3...\n",
      "✅ Baseline CSV found. Running baseline suggestion job...\n",
      "............\u001b[34m2025-02-23 05:27:04.405246: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:04.405277: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:05.980301: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:05.980330: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:05.980349: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-0-88-128.ec2.internal): /proc/driver/nvidia/version does not exist\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:05.980615: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\u001b[0m\n",
      "\u001b[34mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:07,549 - __main__ - INFO - All params:{'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:607916531205:processing-job/baseline-suggestion-job-2025-02-23-05-25-05-194', 'ProcessingJobName': 'baseline-suggestion-job-2025-02-23-05-25-05-194', 'Environment': {'dataset_format': '{\"csv\": {\"header\": false, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}, 'AppSpecification': {'ImageUri': '156813124566.dkr.ecr.us-east-1.amazonaws.com/sagemaker-model-monitor-analyzer', 'ContainerEntrypoint': None, 'ContainerArguments': None}, 'ProcessingInputs': [{'InputName': 'baseline_dataset_input', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/baseline_dataset_input', 'S3Uri': 's3://sagemaker-us-east-1-607916531205/data_capture/flight-delay-xgboost-endpoint-single-request/AllTraffic/fixed_baseline.csv', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinitionInput': None}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'monitoring_output', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/output', 'S3Uri': 's3://sagemaker-us-east-1-607916531205/baseline_results/', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.m5.xlarge', 'VolumeSizeInGB': 30, 'VolumeKmsKeyId': None}}, 'NetworkConfig': {'VpcConfig': None, 'EnableNetworkIsolation': False, 'EnableInterContainerTrafficEncryption': False}, 'RoleArn': 'arn:aws:iam::607916531205:role/LabRole', 'StoppingCondition': {'MaxRuntimeInSeconds': 3600}}\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:07,549 - __main__ - INFO - Current Environment:{'dataset_format': '{\"csv\": {\"header\": false, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:07,549 - __main__ - INFO - categorical_drift_method:None\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:07,549 - DefaultDataAnalyzer - INFO - Performing analysis with input: {\"dataset_source\": \"/opt/ml/processing/input/baseline_dataset_input\", \"dataset_format\": {\"csv\": {\"header\": false, \"output_columns_position\": \"START\"}}, \"output_path\": \"/opt/ml/processing/output\", \"monitoring_input_type\": null, \"analysis_type\": null, \"problem_type\": null, \"inference_attribute\": null, \"probability_attribute\": null, \"ground_truth_attribute\": null, \"probability_threshold_attribute\": null, \"positive_label\": null, \"exclude_features_attribute\": null, \"record_preprocessor_script\": null, \"post_analytics_processor_script\": null, \"baseline_constraints\": null, \"baseline_statistics\": null, \"data_quality_monitoring_config\": {\"evaluate_constraints\": \"Enabled\", \"emit_metrics\": \"Enabled\", \"datatype_check_threshold\": 1.0, \"domain_content_threshold\": 1.0, \"distribution_constraints\": {\"perform_comparison\": \"Enabled\", \"comparison_threshold\": 0.1, \"comparison_method\": \"Robust\", \"categorical_comparison_threshold\": 0.1, \"categorical_drift_method\": \"LInfinity\"}}, \"start_time\": null, \"end_time\": null, \"metric_time\": null, \"cloudwatch_metrics_directory\": \"/opt/ml/output/metrics/cloudwatch\", \"publish_cloudwatch_metrics\": \"Disabled\", \"sagemaker_endpoint_name\": null, \"sagemaker_monitoring_schedule_name\": null, \"output_message_file\": \"/opt/ml/output/message\", \"detect_outliers\": null, \"detect_drift\": null, \"image_data\": null, \"report_enabled\": false, \"auto_ml_job_detail\": null}\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:07,549 - DefaultDataAnalyzer - INFO - Bootstrapping yarn\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:07,549 - bootstrap - INFO - Copy aws jars\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:07,610 - bootstrap - INFO - Copy cluster config\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:07,611 - bootstrap - INFO - Write runtime cluster config\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:07,611 - bootstrap - INFO - Resource Config is: {'current_host': 'algo-1', 'current_instance_type': 'ml.m5.xlarge', 'current_group_name': 'homogeneousCluster', 'hosts': ['algo-1'], 'instance_groups': [{'instance_group_name': 'homogeneousCluster', 'instance_type': 'ml.m5.xlarge', 'hosts': ['algo-1']}], 'network_interface_name': 'eth0'}\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:07,616 - bootstrap - INFO - Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:07,617 - bootstrap - INFO - Starting spark process for master node algo-1\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:07,617 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs namenode -format -force\u001b[0m\n",
      "\u001b[34mWARNING: /usr/hadoop-3.0.0/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,073 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.88.128\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.0.0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/hadoop-3.0.0/etc/hadoop:/usr/hadoop-3.0.0/share/hadoop/common/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/junit-4.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/aws-java-sdk-bundle-1.11.199.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-aws-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-kms-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okio-1.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/yarn:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/joni-2.1.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-annotations-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-prefix-tree-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-compiler-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-el-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/servlet-api-2.5-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-csv-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-common-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-protocol-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-api-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-client-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop2-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-runtime-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-math-2.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/htrace-core-3.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jamon-runtime-2.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/fst-2.50.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jcodings-1.0.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/disruptor-3.3.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-server-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-2.2.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/findbugs-annotations-1.3.9-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-procedure-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-registry-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/\u001b[0m\n",
      "\u001b[34myarn/hadoop-yarn-api-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-router-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-tests-3.0.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r c25427ceca461ee979d30edd7a4b0f50718e6533; compiled by 'andrew' on 2017-12-08T19:16Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_392\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,083 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,088 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-54b711e7-b299-4849-8923-0c54bfa9a8a6\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,661 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,675 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,676 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,679 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,684 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,684 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,684 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,684 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,714 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,726 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,726 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,729 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,732 INFO blockmanagement.BlockManager: The block deletion will start around 2025 Feb 23 05:27:08\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,733 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,734 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,735 INFO util.GSet: 2.0% max memory 3.1 GB = 63.5 MB\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,735 INFO util.GSet: capacity      = 2^23 = 8388608 entries\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,768 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,771 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,772 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,772 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,772 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,772 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,772 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,772 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,772 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,772 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,772 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,772 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,797 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,797 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,797 INFO util.GSet: 1.0% max memory 3.1 GB = 31.7 MB\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,797 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,799 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,799 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,799 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,799 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,803 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,806 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,806 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,807 INFO util.GSet: 0.25% max memory 3.1 GB = 7.9 MB\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,807 INFO util.GSet: capacity      = 2^20 = 1048576 entries\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,843 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,843 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,843 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,847 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,847 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,848 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,848 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,849 INFO util.GSet: 0.029999999329447746% max memory 3.1 GB = 974.9 KB\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,849 INFO util.GSet: capacity      = 2^17 = 131072 entries\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,870 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1881419247-10.0.88.128-1740288428863\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,882 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,889 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,967 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 389 bytes saved in 0 seconds.\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,979 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,982 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.0.88.128\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:08,993 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:11,050 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode, return code 1\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:11,051 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:13,120 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode, return code 1\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:13,121 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:15,194 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager, return code 1\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:15,195 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:17,284 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager, return code 1\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:17,284 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:19,401 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver, return code 1\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:19,402 - DefaultDataAnalyzer - INFO - Total number of hosts in the cluster: 1\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:29,413 - DefaultDataAnalyzer - INFO - Running command: bin/spark-submit --master yarn --deploy-mode client --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider --conf spark.serializer=org.apache.spark.serializer.KryoSerializer /opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:31,106 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:31,538 INFO Main: Start analyzing with args: --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:31,572 INFO Main: Analytics input path: DataAnalyzerParams(/tmp/spark_job_config.json,yarn)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:31,594 INFO FileUtil: Read file from path /tmp/spark_job_config.json.\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:32,125 INFO spark.SparkContext: Running Spark version 3.3.0\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:32,149 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:32,150 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:32,150 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:32,151 INFO spark.SparkContext: Submitted application: SageMakerDataAnalyzer\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:32,174 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 3, script: , vendor: , memory -> name: memory, amount: 11507, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:32,187 INFO resource.ResourceProfile: Limiting resource is cpus at 3 tasks per executor\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:32,188 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:32,235 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:32,236 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:32,236 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:32,236 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:32,237 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:32,529 INFO util.Utils: Successfully started service 'sparkDriver' on port 44507.\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:32,555 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:32,588 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:32,608 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:32,609 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:32,652 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:32,679 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-5e87e20a-ebcc-46ba-a51d-1acae7501ad5\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:32,701 INFO memory.MemoryStore: MemoryStore started with capacity 1458.6 MiB\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:32,752 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:32,797 INFO spark.SparkContext: Added JAR file:/opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar at spark://10.0.88.128:44507/jars/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar with timestamp 1740288452121\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:33,350 INFO client.RMProxy: Connecting to ResourceManager at /10.0.88.128:8032\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:34,011 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:34,012 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:34,018 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (15692 MB per container)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:34,019 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:34,019 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:34,019 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:34,025 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:34,103 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:36,371 INFO yarn.Client: Uploading resource file:/tmp/spark-7719e246-b3ac-4ffe-acb6-005a62e21885/__spark_libs__6351726370878157093.zip -> hdfs://10.0.88.128/user/root/.sparkStaging/application_1740288434697_0001/__spark_libs__6351726370878157093.zip\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:37,511 INFO yarn.Client: Uploading resource file:/tmp/spark-7719e246-b3ac-4ffe-acb6-005a62e21885/__spark_conf__1980840679912670948.zip -> hdfs://10.0.88.128/user/root/.sparkStaging/application_1740288434697_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:37,559 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:37,559 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:37,559 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:37,559 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:37,559 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:37,586 INFO yarn.Client: Submitting application application_1740288434697_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:37,784 INFO impl.YarnClientImpl: Submitted application application_1740288434697_0001\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:38,789 INFO yarn.Client: Application report for application_1740288434697_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:38,792 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: AM container is launched, waiting for AM container to Register with RM\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1740288457686\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1740288434697_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:39,795 INFO yarn.Client: Application report for application_1740288434697_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:40,798 INFO yarn.Client: Application report for application_1740288434697_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:41,801 INFO yarn.Client: Application report for application_1740288434697_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:42,419 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1740288434697_0001), /proxy/application_1740288434697_0001\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:42,809 INFO yarn.Client: Application report for application_1740288434697_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:42,810 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.0.88.128\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1740288457686\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1740288434697_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:42,811 INFO cluster.YarnClientSchedulerBackend: Application application_1740288434697_0001 has started running.\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:42,821 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41189.\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:42,821 INFO netty.NettyBlockTransferService: Server created on 10.0.88.128:41189\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:42,823 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:42,833 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.88.128, 41189, None)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:42,836 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.88.128:41189 with 1458.6 MiB RAM, BlockManagerId(driver, 10.0.88.128, 41189, None)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:42,840 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.88.128, 41189, None)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:42,841 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.88.128, 41189, None)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:42,954 INFO util.log: Logging initialized @13300ms to org.sparkproject.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:43,768 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:46,582 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.88.128:57634) with ID 1,  ResourceProfileId 0\u001b[0m\n",
      "\u001b[34m2025-02-23 05:27:46,806 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-1:45663 with 5.8 GiB RAM, BlockManagerId(1, algo-1, 45663, None)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:03,211 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:03,413 WARN spark.SparkContext: Spark is not running in local mode, therefore the checkpoint directory must not be on the local filesystem. Directory '/tmp' appears to be on the local filesystem.\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:03,466 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:03,471 INFO internal.SharedState: Warehouse path is 'file:/usr/spark-3.3.0/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:04,505 INFO datasources.InMemoryFileIndex: It took 38 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:04,663 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 416.9 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:04,989 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 39.2 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:04,992 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.88.128:41189 (size: 39.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:04,997 INFO spark.SparkContext: Created broadcast 0 from csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:05,371 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:05,374 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:05,377 INFO input.CombineFileInputFormat: DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 5711\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:05,458 INFO spark.SparkContext: Starting job: csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:05,478 INFO scheduler.DAGScheduler: Got job 0 (csv at DatasetReader.scala:99) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:05,479 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (csv at DatasetReader.scala:99)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:05,479 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:05,481 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:05,489 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:05,533 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.3 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:05,536 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:05,537 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.88.128:41189 (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:05,538 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:05,554 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:05,556 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:05,603 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4627 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:05,869 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-1:45663 (size: 4.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:06,714 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-1:45663 (size: 39.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:07,042 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1455 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:07,044 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:07,050 INFO scheduler.DAGScheduler: ResultStage 0 (csv at DatasetReader.scala:99) finished in 1.529 s\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:07,053 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:07,054 INFO cluster.YarnScheduler: Killing all running tasks in stage 0: Stage finished\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:07,055 INFO scheduler.DAGScheduler: Job 0 finished: csv at DatasetReader.scala:99, took 1.596629 s\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:07,206 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.88.128:41189 in memory (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:07,218 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on algo-1:45663 in memory (size: 4.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:09,343 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:09,345 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:09,348 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: string, _c1: string, _c2: string, _c3: string, _c4: string ... 18 more fields>\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:09,532 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 416.5 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:09,547 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 39.1 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:09,547 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.88.128:41189 (size: 39.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:09,549 INFO spark.SparkContext: Created broadcast 2 from head at DataAnalyzer.scala:124\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:09,562 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:09,603 INFO spark.SparkContext: Starting job: head at DataAnalyzer.scala:124\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:09,604 INFO scheduler.DAGScheduler: Got job 1 (head at DataAnalyzer.scala:124) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:09,604 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (head at DataAnalyzer.scala:124)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:09,604 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:09,607 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:09,608 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:124), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:09,660 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 18.7 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:09,662 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.5 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:09,663 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.88.128:41189 (size: 8.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:09,663 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:09,664 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:124) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:09,664 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:09,668 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4955 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:09,713 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-1:45663 (size: 8.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:10,493 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-1:45663 (size: 39.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:10,603 INFO storage.BlockManagerInfo: Added rdd_7_0 in memory on algo-1:45663 (size: 9.6 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:10,768 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1103 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:10,770 INFO scheduler.DAGScheduler: ResultStage 1 (head at DataAnalyzer.scala:124) finished in 1.157 s\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:10,770 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:10,776 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:10,776 INFO cluster.YarnScheduler: Killing all running tasks in stage 1: Stage finished\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:10,776 INFO scheduler.DAGScheduler: Job 1 finished: head at DataAnalyzer.scala:124, took 1.173025 s\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:11,076 INFO codegen.CodeGenerator: Code generated in 226.548111 ms\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:11,583 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:11,719 INFO scheduler.DAGScheduler: Registering RDD 16 (collect at AnalysisRunner.scala:326) as input to shuffle 0\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:11,722 INFO scheduler.DAGScheduler: Got map stage job 2 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:11,722 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:11,722 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:11,724 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:11,726 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:11,744 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 115.5 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:11,746 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 35.4 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:11,747 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.88.128:41189 (size: 35.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:11,748 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:11,749 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:11,750 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:11,757 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4944 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:11,787 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-1:45663 (size: 35.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:12,817 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1062 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:12,817 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:12,819 INFO scheduler.DAGScheduler: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326) finished in 1.091 s\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:12,819 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:12,820 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:12,820 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:12,820 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:12,895 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:12,897 INFO scheduler.DAGScheduler: Got job 3 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:12,897 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:12,897 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:12,897 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:12,899 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:12,911 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 168.0 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:12,913 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 46.2 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:12,914 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.88.128:41189 (size: 46.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:12,914 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:12,915 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:12,915 INFO cluster.YarnScheduler: Adding task set 4.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:12,918 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:12,934 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-1:45663 (size: 46.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:12,971 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.88.128:57634\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:13,340 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 423 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:13,340 INFO cluster.YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:13,342 INFO scheduler.DAGScheduler: ResultStage 4 (collect at AnalysisRunner.scala:326) finished in 0.437 s\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:13,342 INFO scheduler.DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:13,342 INFO cluster.YarnScheduler: Killing all running tasks in stage 4: Stage finished\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:13,342 INFO scheduler.DAGScheduler: Job 3 finished: collect at AnalysisRunner.scala:326, took 0.447519 s\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:13,415 INFO codegen.CodeGenerator: Code generated in 58.051235 ms\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:13,759 INFO codegen.CodeGenerator: Code generated in 43.793609 ms\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:13,847 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:13,849 INFO scheduler.DAGScheduler: Got job 4 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:13,849 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:13,849 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:13,850 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:13,851 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:13,879 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 39.4 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:13,919 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 16.7 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:13,921 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.88.128:41189 (size: 16.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:13,922 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:13,923 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:13,923 INFO cluster.YarnScheduler: Adding task set 5.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:13,931 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4955 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:13,945 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-1:45663 in memory (size: 35.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:13,958 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.88.128:41189 in memory (size: 35.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:13,974 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-1:45663 (size: 16.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:14,027 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.88.128:41189 in memory (size: 8.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:14,045 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-1:45663 in memory (size: 8.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:14,131 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-1:45663 in memory (size: 46.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:14,147 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.88.128:41189 in memory (size: 46.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:14,459 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 530 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:14,459 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:14,460 INFO scheduler.DAGScheduler: ResultStage 5 (treeReduce at KLLRunner.scala:107) finished in 0.607 s\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:14,461 INFO scheduler.DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:14,461 INFO cluster.YarnScheduler: Killing all running tasks in stage 5: Stage finished\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:14,461 INFO scheduler.DAGScheduler: Job 4 finished: treeReduce at KLLRunner.scala:107, took 0.613774 s\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:14,928 INFO codegen.CodeGenerator: Code generated in 108.685891 ms\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:14,936 INFO scheduler.DAGScheduler: Registering RDD 34 (collect at AnalysisRunner.scala:326) as input to shuffle 1\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:14,936 INFO scheduler.DAGScheduler: Got map stage job 5 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:14,936 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:14,936 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:14,937 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:14,938 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:14,947 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 74.9 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:14,949 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 23.7 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:14,950 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.88.128:41189 (size: 23.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:14,950 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:14,951 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:14,951 INFO cluster.YarnScheduler: Adding task set 6.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:14,952 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4944 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:14,965 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-1:45663 (size: 23.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:15,095 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 142 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:15,095 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:15,096 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326) finished in 0.156 s\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:15,097 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:15,097 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:15,097 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:15,097 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:15,441 INFO codegen.CodeGenerator: Code generated in 169.828246 ms\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:15,454 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:15,456 INFO scheduler.DAGScheduler: Got job 6 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:15,456 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:15,456 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:15,456 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:15,458 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:15,460 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 66.2 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:15,462 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 19.1 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:15,463 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.88.128:41189 (size: 19.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:15,465 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:15,465 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:15,465 INFO cluster.YarnScheduler: Adding task set 8.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:15,467 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:15,487 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-1:45663 (size: 19.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:15,492 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.88.128:57634\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:15,653 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 186 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:15,654 INFO cluster.YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:15,655 INFO scheduler.DAGScheduler: ResultStage 8 (collect at AnalysisRunner.scala:326) finished in 0.196 s\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:15,655 INFO scheduler.DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:15,656 INFO cluster.YarnScheduler: Killing all running tasks in stage 8: Stage finished\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:15,666 INFO scheduler.DAGScheduler: Job 6 finished: collect at AnalysisRunner.scala:326, took 0.211160 s\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:15,726 INFO codegen.CodeGenerator: Code generated in 41.073066 ms\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:15,810 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:15,814 INFO scheduler.DAGScheduler: Registering RDD 45 (countByKey at ColumnProfiler.scala:592) as input to shuffle 2\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:15,815 INFO scheduler.DAGScheduler: Got job 7 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:15,815 INFO scheduler.DAGScheduler: Final stage: ResultStage 10 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:15,815 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:15,815 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:15,818 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:15,825 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 31.7 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:15,826 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 14.3 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:15,827 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.88.128:41189 (size: 14.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:15,827 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:15,827 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:15,828 INFO cluster.YarnScheduler: Adding task set 9.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:15,829 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4944 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:15,840 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-1:45663 (size: 14.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,065 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 1236 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,065 INFO cluster.YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,066 INFO scheduler.DAGScheduler: ShuffleMapStage 9 (countByKey at ColumnProfiler.scala:592) finished in 1.248 s\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,066 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,068 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,069 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 10)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,069 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,070 INFO scheduler.DAGScheduler: Submitting ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,072 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 5.1 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,075 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,076 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.88.128:41189 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,078 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,078 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,079 INFO cluster.YarnScheduler: Adding task set 10.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,081 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 8) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,099 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-1:45663 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,106 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.0.88.128:57634\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,163 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 8) in 83 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,163 INFO cluster.YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,164 INFO scheduler.DAGScheduler: ResultStage 10 (countByKey at ColumnProfiler.scala:592) finished in 0.093 s\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,164 INFO scheduler.DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,164 INFO cluster.YarnScheduler: Killing all running tasks in stage 10: Stage finished\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,166 INFO scheduler.DAGScheduler: Job 7 finished: countByKey at ColumnProfiler.scala:592, took 1.355558 s\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,415 INFO scheduler.DAGScheduler: Registering RDD 51 (collect at AnalysisRunner.scala:326) as input to shuffle 3\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,416 INFO scheduler.DAGScheduler: Got map stage job 8 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,416 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,416 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,417 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,419 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,424 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 84.5 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,427 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,427 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.88.128:41189 (size: 27.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,428 INFO spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,429 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,429 INFO cluster.YarnScheduler: Adding task set 11.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,430 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 9) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4944 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,441 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-1:45663 (size: 27.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,609 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 9) in 179 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,609 INFO cluster.YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,610 INFO scheduler.DAGScheduler: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326) finished in 0.190 s\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,610 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,610 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,610 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,610 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,656 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,657 INFO scheduler.DAGScheduler: Got job 9 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,657 INFO scheduler.DAGScheduler: Final stage: ResultStage 13 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,658 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,658 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,659 INFO scheduler.DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,668 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 169.1 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,671 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 46.4 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,671 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.88.128:41189 (size: 46.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,672 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,672 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,672 INFO cluster.YarnScheduler: Adding task set 13.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,673 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 13.0 (TID 10) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,683 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-1:45663 (size: 46.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,691 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 10.0.88.128:57634\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,788 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 13.0 (TID 10) in 114 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,788 INFO cluster.YarnScheduler: Removed TaskSet 13.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,790 INFO scheduler.DAGScheduler: ResultStage 13 (collect at AnalysisRunner.scala:326) finished in 0.130 s\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,790 INFO scheduler.DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,791 INFO cluster.YarnScheduler: Killing all running tasks in stage 13: Stage finished\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,791 INFO scheduler.DAGScheduler: Job 9 finished: collect at AnalysisRunner.scala:326, took 0.134788 s\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,922 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,923 INFO scheduler.DAGScheduler: Got job 10 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,923 INFO scheduler.DAGScheduler: Final stage: ResultStage 14 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,923 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,924 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,925 INFO scheduler.DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,931 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 39.3 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,933 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 16.6 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,933 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.88.128:41189 (size: 16.6 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,933 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,934 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,934 INFO cluster.YarnScheduler: Adding task set 14.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,935 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 11) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4955 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,949 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-1:45663 (size: 16.6 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,964 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 11) in 29 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,964 INFO cluster.YarnScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,965 INFO scheduler.DAGScheduler: ResultStage 14 (treeReduce at KLLRunner.scala:107) finished in 0.039 s\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,965 INFO scheduler.DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,965 INFO cluster.YarnScheduler: Killing all running tasks in stage 14: Stage finished\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:17,966 INFO scheduler.DAGScheduler: Job 10 finished: treeReduce at KLLRunner.scala:107, took 0.043299 s\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,124 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.0.88.128:41189 in memory (size: 19.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,127 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on algo-1:45663 in memory (size: 19.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,130 INFO scheduler.DAGScheduler: Registering RDD 69 (collect at AnalysisRunner.scala:326) as input to shuffle 4\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,131 INFO scheduler.DAGScheduler: Got map stage job 11 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,131 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,131 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,132 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,132 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,142 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 74.9 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,144 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 23.7 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,145 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.88.128:41189 (size: 23.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,148 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 10.0.88.128:41189 in memory (size: 16.6 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,149 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on algo-1:45663 in memory (size: 16.6 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,151 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,151 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,151 INFO cluster.YarnScheduler: Adding task set 15.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,153 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 12) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4944 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,170 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-1:45663 (size: 23.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,191 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 12) in 38 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,191 INFO cluster.YarnScheduler: Removed TaskSet 15.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,192 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on algo-1:45663 in memory (size: 46.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,192 INFO scheduler.DAGScheduler: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326) finished in 0.059 s\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,193 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,193 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,193 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,193 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,198 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.0.88.128:41189 in memory (size: 46.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,226 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 10.0.88.128:41189 in memory (size: 27.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,238 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on algo-1:45663 in memory (size: 27.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,257 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,259 INFO scheduler.DAGScheduler: Got job 12 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,259 INFO scheduler.DAGScheduler: Final stage: ResultStage 17 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,259 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,259 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,259 INFO scheduler.DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,261 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 66.2 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,263 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 19.2 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,264 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.88.128:41189 (size: 19.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,265 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on algo-1:45663 in memory (size: 23.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,266 INFO spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,266 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,267 INFO cluster.YarnScheduler: Adding task set 17.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,268 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.0.88.128:41189 in memory (size: 23.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,268 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 17.0 (TID 13) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,290 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.0.88.128:41189 in memory (size: 14.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,311 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on algo-1:45663 in memory (size: 14.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,311 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-1:45663 (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,320 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.0.88.128:57634\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,327 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 17.0 (TID 13) in 59 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,328 INFO scheduler.DAGScheduler: ResultStage 17 (collect at AnalysisRunner.scala:326) finished in 0.068 s\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,329 INFO cluster.YarnScheduler: Removed TaskSet 17.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,329 INFO scheduler.DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,329 INFO cluster.YarnScheduler: Killing all running tasks in stage 17: Stage finished\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,330 INFO scheduler.DAGScheduler: Job 12 finished: collect at AnalysisRunner.scala:326, took 0.072460 s\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,331 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.0.88.128:41189 in memory (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,332 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on algo-1:45663 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,349 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.0.88.128:41189 in memory (size: 16.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,357 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-1:45663 in memory (size: 16.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,397 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,398 INFO scheduler.DAGScheduler: Registering RDD 80 (countByKey at ColumnProfiler.scala:592) as input to shuffle 5\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,398 INFO scheduler.DAGScheduler: Got job 13 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,398 INFO scheduler.DAGScheduler: Final stage: ResultStage 19 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,398 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 18)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,398 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 18)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,399 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[80] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,406 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 31.7 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,408 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 14.3 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,408 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.88.128:41189 (size: 14.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,409 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,410 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[80] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,410 INFO cluster.YarnScheduler: Adding task set 18.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,411 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 14) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4944 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,422 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-1:45663 (size: 14.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,466 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 14) in 55 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,467 INFO cluster.YarnScheduler: Removed TaskSet 18.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,468 INFO scheduler.DAGScheduler: ShuffleMapStage 18 (countByKey at ColumnProfiler.scala:592) finished in 0.068 s\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,468 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,468 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,469 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 19)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,469 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,469 INFO scheduler.DAGScheduler: Submitting ResultStage 19 (ShuffledRDD[81] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,471 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 5.1 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,473 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,474 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.88.128:41189 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,475 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,476 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (ShuffledRDD[81] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,477 INFO cluster.YarnScheduler: Adding task set 19.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,479 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 19.0 (TID 15) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,490 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-1:45663 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,493 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 10.0.88.128:57634\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,520 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 19.0 (TID 15) in 41 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,520 INFO cluster.YarnScheduler: Removed TaskSet 19.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,521 INFO scheduler.DAGScheduler: ResultStage 19 (countByKey at ColumnProfiler.scala:592) finished in 0.051 s\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,522 INFO scheduler.DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,522 INFO cluster.YarnScheduler: Killing all running tasks in stage 19: Stage finished\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,522 INFO scheduler.DAGScheduler: Job 13 finished: countByKey at ColumnProfiler.scala:592, took 0.125414 s\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,701 INFO scheduler.DAGScheduler: Registering RDD 86 (collect at AnalysisRunner.scala:326) as input to shuffle 6\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,701 INFO scheduler.DAGScheduler: Got map stage job 14 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,701 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 20 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,701 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,701 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,702 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[86] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,706 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 84.5 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,708 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,708 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.88.128:41189 (size: 27.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,708 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,709 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[86] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,709 INFO cluster.YarnScheduler: Adding task set 20.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,710 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 20.0 (TID 16) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4944 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,719 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on algo-1:45663 (size: 27.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,821 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 20.0 (TID 16) in 111 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,822 INFO cluster.YarnScheduler: Removed TaskSet 20.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,823 INFO scheduler.DAGScheduler: ShuffleMapStage 20 (collect at AnalysisRunner.scala:326) finished in 0.119 s\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,823 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,823 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,823 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,823 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,864 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,865 INFO scheduler.DAGScheduler: Got job 15 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,865 INFO scheduler.DAGScheduler: Final stage: ResultStage 22 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,865 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 21)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,865 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,865 INFO scheduler.DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[89] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,870 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 169.2 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,872 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 46.4 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,873 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.88.128:41189 (size: 46.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,873 INFO spark.SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,873 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[89] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,874 INFO cluster.YarnScheduler: Adding task set 22.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,875 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 22.0 (TID 17) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,884 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on algo-1:45663 (size: 46.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:18,891 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 10.0.88.128:57634\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,035 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 22.0 (TID 17) in 161 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,035 INFO cluster.YarnScheduler: Removed TaskSet 22.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,037 INFO scheduler.DAGScheduler: ResultStage 22 (collect at AnalysisRunner.scala:326) finished in 0.171 s\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,038 INFO scheduler.DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,038 INFO cluster.YarnScheduler: Killing all running tasks in stage 22: Stage finished\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,038 INFO scheduler.DAGScheduler: Job 15 finished: collect at AnalysisRunner.scala:326, took 0.174516 s\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,156 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,158 INFO scheduler.DAGScheduler: Got job 16 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,158 INFO scheduler.DAGScheduler: Final stage: ResultStage 23 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,158 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,159 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,160 INFO scheduler.DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[99] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,166 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 39.3 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,167 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 16.7 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,168 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.0.88.128:41189 (size: 16.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,169 INFO spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,169 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[99] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,169 INFO cluster.YarnScheduler: Adding task set 23.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,171 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 23.0 (TID 18) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4955 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,181 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on algo-1:45663 (size: 16.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,195 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 23.0 (TID 18) in 25 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,196 INFO cluster.YarnScheduler: Removed TaskSet 23.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,196 INFO scheduler.DAGScheduler: ResultStage 23 (treeReduce at KLLRunner.scala:107) finished in 0.035 s\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,197 INFO scheduler.DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,197 INFO cluster.YarnScheduler: Killing all running tasks in stage 23: Stage finished\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,197 INFO scheduler.DAGScheduler: Job 16 finished: treeReduce at KLLRunner.scala:107, took 0.040599 s\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,400 INFO scheduler.DAGScheduler: Registering RDD 104 (collect at AnalysisRunner.scala:326) as input to shuffle 7\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,401 INFO scheduler.DAGScheduler: Got map stage job 17 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,401 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 24 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,401 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,402 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,402 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 24 (MapPartitionsRDD[104] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,407 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 74.9 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,408 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 23.7 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,409 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.88.128:41189 (size: 23.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,410 INFO spark.SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,418 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[104] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,418 INFO cluster.YarnScheduler: Adding task set 24.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,420 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 24.0 (TID 19) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4944 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,433 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on algo-1:45663 (size: 23.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,453 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 24.0 (TID 19) in 34 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,453 INFO cluster.YarnScheduler: Removed TaskSet 24.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,454 INFO scheduler.DAGScheduler: ShuffleMapStage 24 (collect at AnalysisRunner.scala:326) finished in 0.051 s\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,455 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,456 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,456 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,456 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,527 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,529 INFO scheduler.DAGScheduler: Got job 18 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,530 INFO scheduler.DAGScheduler: Final stage: ResultStage 26 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,530 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 25)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,530 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,530 INFO scheduler.DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[107] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,532 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 66.2 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,535 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 19.2 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,535 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.0.88.128:41189 (size: 19.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,536 INFO spark.SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,536 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[107] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,536 INFO cluster.YarnScheduler: Adding task set 26.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,537 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 26.0 (TID 20) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,554 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on algo-1:45663 (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,561 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.0.88.128:57634\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,571 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 26.0 (TID 20) in 34 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,571 INFO cluster.YarnScheduler: Removed TaskSet 26.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,572 INFO scheduler.DAGScheduler: ResultStage 26 (collect at AnalysisRunner.scala:326) finished in 0.041 s\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,572 INFO scheduler.DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,573 INFO cluster.YarnScheduler: Killing all running tasks in stage 26: Stage finished\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,573 INFO scheduler.DAGScheduler: Job 18 finished: collect at AnalysisRunner.scala:326, took 0.044946 s\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,658 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,659 INFO scheduler.DAGScheduler: Registering RDD 115 (countByKey at ColumnProfiler.scala:592) as input to shuffle 8\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,660 INFO scheduler.DAGScheduler: Got job 19 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,660 INFO scheduler.DAGScheduler: Final stage: ResultStage 28 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,660 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 27)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,660 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 27)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,661 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[115] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,669 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 31.7 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,672 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 14.3 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,673 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.0.88.128:41189 (size: 14.3 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,674 INFO spark.SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,674 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[115] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,674 INFO cluster.YarnScheduler: Adding task set 27.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,675 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 27.0 (TID 21) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4944 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,688 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on algo-1:45663 (size: 14.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,740 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 27.0 (TID 21) in 65 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,740 INFO cluster.YarnScheduler: Removed TaskSet 27.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,741 INFO scheduler.DAGScheduler: ShuffleMapStage 27 (countByKey at ColumnProfiler.scala:592) finished in 0.079 s\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,744 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,745 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,745 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 28)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,745 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,745 INFO scheduler.DAGScheduler: Submitting ResultStage 28 (ShuffledRDD[116] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,747 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 5.1 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,748 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,750 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.0.88.128:41189 (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,751 INFO spark.SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,751 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (ShuffledRDD[116] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,751 INFO cluster.YarnScheduler: Adding task set 28.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,752 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 28.0 (TID 22) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,766 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on algo-1:45663 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,771 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to 10.0.88.128:57634\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,790 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 28.0 (TID 22) in 38 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,790 INFO cluster.YarnScheduler: Removed TaskSet 28.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,791 INFO scheduler.DAGScheduler: ResultStage 28 (countByKey at ColumnProfiler.scala:592) finished in 0.045 s\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,792 INFO scheduler.DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,792 INFO cluster.YarnScheduler: Killing all running tasks in stage 28: Stage finished\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,795 INFO scheduler.DAGScheduler: Job 19 finished: countByKey at ColumnProfiler.scala:592, took 0.136630 s\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,914 INFO scheduler.DAGScheduler: Registering RDD 121 (collect at AnalysisRunner.scala:326) as input to shuffle 9\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,915 INFO scheduler.DAGScheduler: Got map stage job 20 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,915 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 29 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,915 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,917 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,920 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 29 (MapPartitionsRDD[121] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,923 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 84.5 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,925 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 27.6 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,926 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.0.88.128:41189 (size: 27.6 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,928 INFO spark.SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,928 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 29 (MapPartitionsRDD[121] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,928 INFO cluster.YarnScheduler: Adding task set 29.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,930 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 29.0 (TID 23) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4944 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:19,941 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on algo-1:45663 (size: 27.6 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,075 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 29.0 (TID 23) in 145 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,076 INFO cluster.YarnScheduler: Removed TaskSet 29.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,076 INFO scheduler.DAGScheduler: ShuffleMapStage 29 (collect at AnalysisRunner.scala:326) finished in 0.156 s\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,077 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,078 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,078 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,078 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,123 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,125 INFO scheduler.DAGScheduler: Got job 21 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,125 INFO scheduler.DAGScheduler: Final stage: ResultStage 31 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,125 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 30)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,125 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,126 INFO scheduler.DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[124] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,132 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 169.2 KiB, free 1456.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,137 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 46.4 KiB, free 1456.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,137 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.0.88.128:41189 (size: 46.4 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,138 INFO spark.SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,138 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[124] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,139 INFO cluster.YarnScheduler: Adding task set 31.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,140 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 31.0 (TID 24) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,153 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on algo-1:45663 (size: 46.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,168 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 9 to 10.0.88.128:57634\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,235 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on algo-1:45663 in memory (size: 23.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,236 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on 10.0.88.128:41189 in memory (size: 23.7 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,289 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on algo-1:45663 in memory (size: 14.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,290 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.0.88.128:41189 in memory (size: 14.3 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,315 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on 10.0.88.128:41189 in memory (size: 19.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,317 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on algo-1:45663 in memory (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,341 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on 10.0.88.128:41189 in memory (size: 14.3 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,354 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on algo-1:45663 in memory (size: 14.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,387 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on 10.0.88.128:41189 in memory (size: 46.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,387 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on algo-1:45663 in memory (size: 46.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,398 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 31.0 (TID 24) in 258 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,398 INFO cluster.YarnScheduler: Removed TaskSet 31.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,401 INFO scheduler.DAGScheduler: ResultStage 31 (collect at AnalysisRunner.scala:326) finished in 0.275 s\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,406 INFO scheduler.DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,406 INFO cluster.YarnScheduler: Killing all running tasks in stage 31: Stage finished\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,406 INFO scheduler.DAGScheduler: Job 21 finished: collect at AnalysisRunner.scala:326, took 0.282969 s\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,427 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on 10.0.88.128:41189 in memory (size: 27.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,429 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on algo-1:45663 in memory (size: 27.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,504 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on algo-1:45663 in memory (size: 23.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,511 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.0.88.128:41189 in memory (size: 23.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,513 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,514 INFO scheduler.DAGScheduler: Got job 22 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,514 INFO scheduler.DAGScheduler: Final stage: ResultStage 32 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,514 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,515 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,515 INFO scheduler.DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[134] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,533 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 39.3 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,535 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 16.7 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,535 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.0.88.128:41189 (size: 16.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,536 INFO spark.SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,538 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[134] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,538 INFO cluster.YarnScheduler: Adding task set 32.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,540 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 32.0 (TID 25) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4955 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,542 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on algo-1:45663 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,546 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on 10.0.88.128:41189 in memory (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,549 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on algo-1:45663 (size: 16.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,556 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on algo-1:45663 in memory (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,560 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 10.0.88.128:41189 in memory (size: 19.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,566 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 32.0 (TID 25) in 27 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,566 INFO cluster.YarnScheduler: Removed TaskSet 32.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,567 INFO scheduler.DAGScheduler: ResultStage 32 (treeReduce at KLLRunner.scala:107) finished in 0.051 s\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,567 INFO scheduler.DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,567 INFO cluster.YarnScheduler: Killing all running tasks in stage 32: Stage finished\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,568 INFO scheduler.DAGScheduler: Job 22 finished: treeReduce at KLLRunner.scala:107, took 0.054766 s\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,591 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on algo-1:45663 in memory (size: 16.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,592 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on 10.0.88.128:41189 in memory (size: 16.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,632 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on 10.0.88.128:41189 in memory (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,634 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on algo-1:45663 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,641 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on 10.0.88.128:41189 in memory (size: 27.6 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,643 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on algo-1:45663 in memory (size: 27.6 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,677 INFO scheduler.DAGScheduler: Registering RDD 139 (collect at AnalysisRunner.scala:326) as input to shuffle 10\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,678 INFO scheduler.DAGScheduler: Got map stage job 23 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,678 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 33 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,678 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,678 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,678 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 33 (MapPartitionsRDD[139] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,682 INFO memory.MemoryStore: Block broadcast_28 stored as values in memory (estimated size 74.9 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,683 INFO memory.MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 23.7 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,684 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.0.88.128:41189 (size: 23.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,684 INFO spark.SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,684 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 33 (MapPartitionsRDD[139] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,685 INFO cluster.YarnScheduler: Adding task set 33.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,686 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 33.0 (TID 26) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4944 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,696 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on algo-1:45663 (size: 23.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,707 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 33.0 (TID 26) in 21 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,707 INFO cluster.YarnScheduler: Removed TaskSet 33.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,707 INFO scheduler.DAGScheduler: ShuffleMapStage 33 (collect at AnalysisRunner.scala:326) finished in 0.028 s\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,708 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,708 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,708 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,708 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,754 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,755 INFO scheduler.DAGScheduler: Got job 24 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,755 INFO scheduler.DAGScheduler: Final stage: ResultStage 35 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,755 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 34)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,755 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,755 INFO scheduler.DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[142] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,757 INFO memory.MemoryStore: Block broadcast_29 stored as values in memory (estimated size 66.2 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,760 INFO memory.MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 19.2 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,761 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.0.88.128:41189 (size: 19.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,761 INFO spark.SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,762 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[142] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,762 INFO cluster.YarnScheduler: Adding task set 35.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,763 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 35.0 (TID 27) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,774 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on algo-1:45663 (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,780 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 10 to 10.0.88.128:57634\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,799 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 35.0 (TID 27) in 36 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,799 INFO cluster.YarnScheduler: Removed TaskSet 35.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,800 INFO scheduler.DAGScheduler: ResultStage 35 (collect at AnalysisRunner.scala:326) finished in 0.044 s\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,800 INFO scheduler.DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,800 INFO cluster.YarnScheduler: Killing all running tasks in stage 35: Stage finished\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,801 INFO scheduler.DAGScheduler: Job 24 finished: collect at AnalysisRunner.scala:326, took 0.046530 s\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,878 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,879 INFO scheduler.DAGScheduler: Registering RDD 150 (countByKey at ColumnProfiler.scala:592) as input to shuffle 11\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,880 INFO scheduler.DAGScheduler: Got job 25 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,880 INFO scheduler.DAGScheduler: Final stage: ResultStage 37 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,881 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 36)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,881 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 36)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,882 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 36 (MapPartitionsRDD[150] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,892 INFO memory.MemoryStore: Block broadcast_30 stored as values in memory (estimated size 31.7 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,896 INFO memory.MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 14.3 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,900 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on 10.0.88.128:41189 (size: 14.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,901 INFO spark.SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,901 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 36 (MapPartitionsRDD[150] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,902 INFO cluster.YarnScheduler: Adding task set 36.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,903 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 36.0 (TID 28) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4944 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,913 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on algo-1:45663 (size: 14.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,971 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 36.0 (TID 28) in 68 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,972 INFO cluster.YarnScheduler: Removed TaskSet 36.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,974 INFO scheduler.DAGScheduler: ShuffleMapStage 36 (countByKey at ColumnProfiler.scala:592) finished in 0.091 s\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,974 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,974 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,974 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 37)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,974 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,974 INFO scheduler.DAGScheduler: Submitting ResultStage 37 (ShuffledRDD[151] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,976 INFO memory.MemoryStore: Block broadcast_31 stored as values in memory (estimated size 5.1 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,977 INFO memory.MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,978 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on 10.0.88.128:41189 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,978 INFO spark.SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,979 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 37 (ShuffledRDD[151] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,979 INFO cluster.YarnScheduler: Adding task set 37.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:20,980 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 37.0 (TID 29) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,001 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on algo-1:45663 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,004 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 11 to 10.0.88.128:57634\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,035 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 37.0 (TID 29) in 55 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,035 INFO cluster.YarnScheduler: Removed TaskSet 37.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,035 INFO scheduler.DAGScheduler: ResultStage 37 (countByKey at ColumnProfiler.scala:592) finished in 0.060 s\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,036 INFO scheduler.DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,036 INFO cluster.YarnScheduler: Killing all running tasks in stage 37: Stage finished\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,036 INFO scheduler.DAGScheduler: Job 25 finished: countByKey at ColumnProfiler.scala:592, took 0.158222 s\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,223 INFO FileUtil: Write to file constraints.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,256 INFO codegen.CodeGenerator: Code generated in 10.088198 ms\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,262 INFO scheduler.DAGScheduler: Registering RDD 156 (count at StatsGenerator.scala:66) as input to shuffle 12\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,262 INFO scheduler.DAGScheduler: Got map stage job 26 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,262 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 38 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,262 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,263 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,264 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 38 (MapPartitionsRDD[156] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,266 INFO memory.MemoryStore: Block broadcast_32 stored as values in memory (estimated size 23.7 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,268 INFO memory.MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,268 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on 10.0.88.128:41189 (size: 10.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,268 INFO spark.SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,269 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 38 (MapPartitionsRDD[156] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,269 INFO cluster.YarnScheduler: Adding task set 38.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,270 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 38.0 (TID 30) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4944 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,278 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on algo-1:45663 (size: 10.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,307 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 38.0 (TID 30) in 37 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,307 INFO cluster.YarnScheduler: Removed TaskSet 38.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,308 INFO scheduler.DAGScheduler: ShuffleMapStage 38 (count at StatsGenerator.scala:66) finished in 0.044 s\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,308 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,308 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,308 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,309 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,323 INFO codegen.CodeGenerator: Code generated in 8.615368 ms\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,333 INFO spark.SparkContext: Starting job: count at StatsGenerator.scala:66\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,334 INFO scheduler.DAGScheduler: Got job 27 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,334 INFO scheduler.DAGScheduler: Final stage: ResultStage 40 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,334 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 39)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,334 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,335 INFO scheduler.DAGScheduler: Submitting ResultStage 40 (MapPartitionsRDD[159] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,336 INFO memory.MemoryStore: Block broadcast_33 stored as values in memory (estimated size 11.1 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,338 INFO memory.MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,339 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on 10.0.88.128:41189 (size: 5.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,339 INFO spark.SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,339 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 40 (MapPartitionsRDD[159] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,340 INFO cluster.YarnScheduler: Adding task set 40.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,341 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 40.0 (TID 31) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,353 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on algo-1:45663 (size: 5.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,358 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 12 to 10.0.88.128:57634\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,381 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 40.0 (TID 31) in 40 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,381 INFO cluster.YarnScheduler: Removed TaskSet 40.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,382 INFO scheduler.DAGScheduler: ResultStage 40 (count at StatsGenerator.scala:66) finished in 0.046 s\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,383 INFO scheduler.DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,383 INFO cluster.YarnScheduler: Killing all running tasks in stage 40: Stage finished\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,383 INFO scheduler.DAGScheduler: Job 27 finished: count at StatsGenerator.scala:66, took 0.050225 s\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,598 INFO FileUtil: Write to file statistics.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,619 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,679 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,680 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,689 INFO cluster.YarnClientSchedulerBackend: YARN client scheduler backend Stopped\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,708 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,746 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,746 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,761 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,764 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,836 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,836 INFO Main: Completed: Job completed successfully with no violations.\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,837 INFO Main: Write to file /opt/ml/output/message.\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,859 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,860 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-8bc090e5-ea67-42d6-8460-aa6f15b54eae\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,876 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-7719e246-b3ac-4ffe-acb6-005a62e21885\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,935 - DefaultDataAnalyzer - INFO - Completed spark-submit with return code : 0\u001b[0m\n",
      "\u001b[34m2025-02-23 05:28:21,935 - DefaultDataAnalyzer - INFO - Spark job completed.\u001b[0m\n",
      "\n",
      "✅ Baseline job completed. Results saved at: s3://sagemaker-us-east-1-607916531205/baseline_results/\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "try:\n",
    "    print(\"🔍 Checking if processed baseline CSV exists in S3...\")\n",
    "\n",
    "    # ✅ Define the fixed CSV file path in S3\n",
    "    fixed_s3_path = f\"{baseline_dataset_path}fixed_baseline.csv\"\n",
    "\n",
    "    # ✅ Check if the CSV file exists in S3\n",
    "    response = s3_client.list_objects_v2(Bucket=bucket, Prefix=fixed_s3_path.replace(f\"s3://{bucket}/\", \"\"))\n",
    "\n",
    "    if \"Contents\" not in response or not response[\"Contents\"]:\n",
    "        raise FileNotFoundError(f\"⚠️ No processed CSV found at {fixed_s3_path}. Ensure JSONL was converted and uploaded.\")\n",
    "\n",
    "    print(\"✅ Baseline CSV found. Running baseline suggestion job...\")\n",
    "\n",
    "    # ✅ Run baseline generation using CSV format\n",
    "    model_monitor.suggest_baseline(\n",
    "        baseline_dataset=fixed_s3_path,  \n",
    "        dataset_format=DatasetFormat.csv(header=False), \n",
    "        output_s3_uri=baseline_results_uri,  # Where baseline results will be stored\n",
    "        wait=True  # Wait for job completion\n",
    "    )\n",
    "\n",
    "    print(f\"✅ Baseline job completed. Results saved at: {baseline_results_uri}\")\n",
    "\n",
    "except FileNotFoundError as fe:\n",
    "    print(str(fe))  # Print specific missing file error\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error running baseline suggestion job: {e}\")\n",
    "    raise  # Re-raise the error for debugging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c817d72d-6279-41b3-8574-2a25ab390331",
   "metadata": {},
   "source": [
    "### Verify Baseline Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c80d998f-0d8e-430e-9fe7-1ed966dce82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 baseline_results/constraints.json - 3519 bytes\n",
      "📂 baseline_results/statistics.json - 41918 bytes\n"
     ]
    }
   ],
   "source": [
    "# ✅ Check the size of statistics.json and constraints.json in S3\n",
    "response = s3_client.list_objects_v2(Bucket=bucket, Prefix=baseline_results_uri.replace(f\"s3://{bucket}/\", \"\"))\n",
    "\n",
    "if \"Contents\" in response:\n",
    "    for obj in response[\"Contents\"]:\n",
    "        file_name = obj[\"Key\"]\n",
    "        file_size = obj[\"Size\"]  # Size in bytes\n",
    "        print(f\"📂 {file_name} - {file_size} bytes\")\n",
    "\n",
    "        if file_size == 0:\n",
    "            print(f\"⚠️ WARNING: {file_name} is empty! The baseline job may not have written data correctly.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7dcd8026-9e88-40e1-a329-5d9c78fa5c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.httpchecksum:Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline statistics stored at: s3://sagemaker-us-east-1-607916531205/baseline_results/statistics.json\n",
      "✅ statistics.json successfully retrieved using `get_object()`.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# ✅ Define S3 path\n",
    "baseline_statistics_path = f\"{baseline_results_uri}statistics.json\"\n",
    "print(\"Baseline statistics stored at:\", baseline_statistics_path)\n",
    "\n",
    "# ✅ Try downloading `statistics.json` using `get_object()` instead\n",
    "try:\n",
    "    response = s3_client.get_object(Bucket=bucket, Key=baseline_statistics_path.replace(f\"s3://{bucket}/\", \"\"))\n",
    "    with open(\"statistics.json\", \"wb\") as f:\n",
    "        f.write(response[\"Body\"].read())\n",
    "    print(\"✅ statistics.json successfully retrieved using `get_object()`.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to retrieve statistics.json: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b2f9a158-d1d9-4683-baaf-d800adb85656",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.httpchecksum:Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline constraints stored at: s3://sagemaker-us-east-1-607916531205/baseline_results/constraints.json\n",
      "✅ constraints.json successfully retrieved using `get_object()`.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# ✅ Define S3 path\n",
    "baseline_constraints_path = f\"{baseline_results_uri}constraints.json\"\n",
    "print(\"Baseline constraints stored at:\", baseline_constraints_path)\n",
    "\n",
    "# ✅ Try downloading `constraints.json` using `get_object()`\n",
    "try:\n",
    "    response = s3_client.get_object(Bucket=bucket, Key=baseline_constraints_path.replace(f\"s3://{bucket}/\", \"\"))\n",
    "    with open(\"constraints.json\", \"wb\") as f:\n",
    "        f.write(response[\"Body\"].read())\n",
    "    print(\"✅ constraints.json successfully retrieved using `get_object()`.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to retrieve constraints.json: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "94691c51-9338-4a5c-a782-1bd3ab184d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sample Drift Thresholds: {'_c0': 1740273202.9792986, '_c1': 2017.4840923430359, '_c2': 8.937354648979131, '_c3': 10.647831917092148, '_c4': 264.0408222704953}\n",
      "✅ Total features with drift thresholds: 20\n",
      "✅ Constraints contain 20 feature checks.\n",
      "Stored 'drift_thresholds' (dict)\n",
      "✅ Stored drift thresholds in %store for CloudWatch Alarm.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# ✅ Load baseline statistics\n",
    "with open(\"statistics.json\", \"r\") as stats_file:\n",
    "    baseline_statistics = json.load(stats_file)\n",
    "\n",
    "# ✅ Extract drift thresholds (mean + 2 * std_dev)\n",
    "drift_thresholds = {}\n",
    "for feature in baseline_statistics[\"features\"]:\n",
    "    feature_name = feature[\"name\"]\n",
    "    mean = feature[\"numerical_statistics\"][\"mean\"]\n",
    "    std_dev = feature[\"numerical_statistics\"][\"std_dev\"]\n",
    "    drift_thresholds[feature_name] = mean + (2 * std_dev)  # ✅ Mean + 2*std_dev threshold\n",
    "\n",
    "# ✅ Print only a sample of drift thresholds (first 5)\n",
    "print(\"✅ Sample Drift Thresholds:\", dict(list(drift_thresholds.items())[:5]))\n",
    "print(f\"✅ Total features with drift thresholds: {len(drift_thresholds)}\")\n",
    "\n",
    "# ✅ Load baseline constraints\n",
    "with open(\"constraints.json\", \"r\") as constraints_file:\n",
    "    baseline_constraints = json.load(constraints_file)\n",
    "\n",
    "# ✅ Print summary of constraints instead of full JSON\n",
    "print(f\"✅ Constraints contain {len(baseline_constraints['features'])} feature checks.\")\n",
    "\n",
    "# ✅ Store the extracted thresholds for later use\n",
    "%store drift_thresholds\n",
    "print(\"✅ Stored drift thresholds in %store for CloudWatch Alarm.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f6d4c4-a5f1-4d11-86a9-37d01c3fbaa8",
   "metadata": {},
   "source": [
    "## Make CloudWatch Alarm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "15e2df52-8e4f-4b7b-90db-9a7b0f74b926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CloudWatch Alarm 'FlightDelayModelDriftAlarm' created successfully with threshold 13947.141033916032.\n",
      "🚀 The feature '_c14' has the 90th percentile drift threshold: 13947.141033916032\n",
      "Stored 'model_drift_alarm_name' (str)\n",
      "✅ Stored model_drift_alarm_name in %store.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ✅ Retrieve stored drift thresholds\n",
    "%store -r drift_thresholds\n",
    "\n",
    "# ✅ Remove '_c0' and any timestamp-like values\n",
    "filtered_thresholds = {k: v for k, v in drift_thresholds.items() if k != \"_c0\" and v < 1e6}\n",
    "\n",
    "# ✅ Compute the 90th percentile threshold (if at least one valid feature remains)\n",
    "if filtered_thresholds:\n",
    "    threshold_value = np.percentile(list(filtered_thresholds.values()), 90)\n",
    "    # ✅ Identify which feature corresponds to this threshold\n",
    "    selected_feature = max(filtered_thresholds, key=lambda k: abs(filtered_thresholds[k] - threshold_value))\n",
    "else:\n",
    "    threshold_value = 1.0  # Default fallback value\n",
    "    selected_feature = \"No valid feature found\"\n",
    "\n",
    "# ✅ Initialize CloudWatch client\n",
    "cw_client = boto3.client(\"cloudwatch\")\n",
    "\n",
    "# ✅ Define CloudWatch Alarm\n",
    "model_drift_alarm_name = \"FlightDelayModelDriftAlarm\"\n",
    "\n",
    "response = cw_client.put_metric_alarm(\n",
    "    AlarmName=model_drift_alarm_name,\n",
    "    AlarmDescription=\"Triggers when model drift is detected in the monitoring schedule.\",\n",
    "    ActionsEnabled=True,\n",
    "    MetricName=\"FeatureAttributeDrift\",\n",
    "    Namespace=\"aws/sagemaker/Endpoints/model-metrics\",\n",
    "    Statistic=\"Average\",\n",
    "    Dimensions=[\n",
    "        {\"Name\": \"Endpoint\", \"Value\": \"flight-delay-xgboost-endpoint-single-request\"},\n",
    "        {\"Name\": \"MonitoringSchedule\", \"Value\": \"FlightDelayMonitor\"},\n",
    "    ],\n",
    "    Period=3600,  # Every hour\n",
    "    EvaluationPeriods=1,\n",
    "    DatapointsToAlarm=1,\n",
    "    Threshold=threshold_value,  # ✅ Now using a corrected 90th percentile threshold\n",
    "    ComparisonOperator=\"GreaterThanThreshold\",\n",
    "    TreatMissingData=\"breaching\",\n",
    ")\n",
    "\n",
    "print(f\"✅ CloudWatch Alarm '{model_drift_alarm_name}' created successfully with threshold {threshold_value}.\")\n",
    "print(f\"🚀 The feature '{selected_feature}' has the 90th percentile drift threshold: {threshold_value}\")\n",
    "\n",
    "%store model_drift_alarm_name\n",
    "print(\"✅ Stored model_drift_alarm_name in %store.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d229f09c-0a26-41fd-b0f1-a0a5b38d7212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 `_c14` corresponds to feature: carrier_delay\n"
     ]
    }
   ],
   "source": [
    "# ✅ Define feature names manually (from the feature store)\n",
    "feature_names = [\n",
    "    \"year\", \"month\", \"carrier\", \"airport\", \"arr_flights\", \"arr_del15\", \"carrier_ct\", \"weather_ct\",\n",
    "    \"nas_ct\", \"security_ct\", \"late_aircraft_ct\", \"arr_cancelled\", \"arr_diverted\", \"arr_delay\",\n",
    "    \"carrier_delay\", \"weather_delay\", \"nas_delay\", \"security_delay\", \"late_aircraft_delay\",\n",
    "    \"delay_rate\", \"on_time\", \"event_time\", \"record_id\"\n",
    "]\n",
    "\n",
    "# ✅ Ensure drift_thresholds uses proper feature names\n",
    "%store -r drift_thresholds\n",
    "\n",
    "# ✅ Map `_c14` to actual feature name\n",
    "mapped_feature_name = feature_names[14]  # Index 14 in the actual dataset\n",
    "print(f\"🚀 `_c14` corresponds to feature: {mapped_feature_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d94c6a-b6c3-47a3-af3e-45ad2700c904",
   "metadata": {},
   "source": [
    "## Setup Sage Model Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3a0622ad-3269-4246-aaba-b33f6cc0804f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: .\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Results Path: s3://sagemaker-us-east-1-607916531205/baseline_results/\n",
      "Monitoring Results Path: s3://sagemaker-us-east-1-607916531205/monitoring_results/\n",
      "⚠️ Monitoring schedule 'FlightDelayMonitor' already exists. Skipping creation.\n",
      "Stored 'flight_delay_drift_monitor_schedule_name' (str)\n",
      "✅ Stored Drift monitor schedule name in %store.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.model_monitor import DefaultModelMonitor, CronExpressionGenerator, EndpointInput\n",
    "\n",
    "# ✅ Define S3 paths\n",
    "monitoring_results_uri = f\"s3://{bucket}/monitoring_results/\"\n",
    "\n",
    "print(f\"Baseline Results Path: {baseline_results_uri}\")\n",
    "print(f\"Monitoring Results Path: {monitoring_results_uri}\")\n",
    "\n",
    "# ✅ Configure the model monitor\n",
    "model_monitor = DefaultModelMonitor(\n",
    "    role=role,  # ✅ Use role from %store\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    max_runtime_in_seconds=3600,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "# ✅ Define Endpoint Input (without dataset_format)\n",
    "endpoint_input = EndpointInput(\n",
    "    endpoint_name=\"flight-delay-xgboost-endpoint-single-request\",\n",
    "    destination=\"/opt/ml/processing/input\"\n",
    ")\n",
    "\n",
    "# ✅ Monitoring Schedule Name\n",
    "flight_delay_drift_monitor_schedule_name = \"FlightDelayMonitor\"\n",
    "\n",
    "# ✅ Check if the schedule already exists (to avoid errors)\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "existing_schedules = sm_client.list_monitoring_schedules()[\"MonitoringScheduleSummaries\"]\n",
    "schedule_names = [schedule[\"MonitoringScheduleName\"] for schedule in existing_schedules]\n",
    "\n",
    "if flight_delay_drift_monitor_schedule_name in schedule_names:\n",
    "    print(f\"⚠️ Monitoring schedule '{flight_delay_drift_monitor_schedule_name}' already exists. Skipping creation.\")\n",
    "else:\n",
    "    # ✅ Schedule Data Monitoring (Every Hour)\n",
    "    model_monitor.create_monitoring_schedule(\n",
    "        monitor_schedule_name=flight_delay_drift_monitor_schedule_name,\n",
    "        endpoint_input=endpoint_input,  # ✅ Fixed: Properly pass EndpointInput\n",
    "        output_s3_uri=monitoring_results_uri,\n",
    "        schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "        enable_cloudwatch_metrics=True,\n",
    "    )\n",
    "    print(f\"✅ Monitoring schedule created for endpoint: flight-delay-xgboost-endpoint-single-request\")\n",
    "\n",
    "# ✅ Store the monitoring schedule names for later use\n",
    "%store flight_delay_drift_monitor_schedule_name\n",
    "print(\"✅ Stored Drift monitor schedule name in %store.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce29f379-a170-4f37-88e3-d8fcab0ad5c6",
   "metadata": {},
   "source": [
    "## Send Fake Data To Trigger Alarm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "35ee33c2-7230-4b78-a8e7-f0245dc817dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CloudWatch Alarm 'FlightDelayModelDriftAlarm' reset to OK state.\n"
     ]
    }
   ],
   "source": [
    "cw_client.set_alarm_state(\n",
    "    AlarmName=alarm_name,\n",
    "    StateValue=\"OK\",\n",
    "    StateReason=\"Resetting alarm to OK state manually.\"\n",
    ")\n",
    "print(f\"✅ CloudWatch Alarm '{alarm_name}' reset to OK state.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e12b019d-b9ba-4da6-9391-b9f295e26bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fake data generated and saved to fake_flight_data.csv\n",
      "🟢 Input: 2007.0,11.0,2.0,1.0,3.0,413.0,83.0,3.0,11.0,6.0,27.0,0.0,0.0,3.0,19.38304602789831,53.842418191692886,10.762279422655618,46.16500556744988,30.895848156704897,22.469515756986723,1.186644095262146\n",
      "🔵 Prediction: b'1.0346633644076064e-05\\n'\n",
      "🟢 Input: 2022.0,6.0,1.0,0.0,5.0,525.0,135.0,39.0,28.0,38.0,30.0,12.0,8.0,7.0,44.23849678625918,33.84863654775507,43.051928988872525,69.22475392469244,16.92049677902927,88.13869527464779,1.702360247056779\n",
      "🔵 Prediction: b'4.337779500929173e-06\\n'\n",
      "🟢 Input: 2017.0,6.0,20.0,1.0,1.0,283.0,97.0,2.0,45.0,14.0,18.0,30.0,2.0,7.0,165.48589029039456,17.098792401646524,70.19742395014109,0.6440027865475373,40.66715806131285,78.28079551855637,1.9780693393943585\n",
      "🔵 Prediction: b'4.337779500929173e-06\\n'\n",
      "🟢 Input: 2003.0,3.0,15.0,2.0,4.0,571.0,30.0,12.0,22.0,31.0,45.0,32.0,0.0,6.0,17.40086510779658,55.07227099860814,94.09834394912916,2.4375308366564874,2.7927741138372157,45.03554364538015,1.690335063434221\n",
      "🔵 Prediction: b'1.0346633644076064e-05\\n'\n",
      "🟢 Input: 2001.0,11.0,25.0,1.0,5.0,232.0,67.0,0.0,28.0,38.0,9.0,26.0,6.0,8.0,78.90762555824303,31.33410796319852,50.83082180698234,15.901658864000137,90.88495981256672,49.77528097947853,1.7008751611908086\n",
      "🔵 Prediction: b'4.337779500929173e-06\\n'\n",
      "✅ Sent 500 rows to the endpoint. Check CloudWatch for drift alerts.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ✅ Feature columns (ensure these match training data)\n",
    "columns = [\n",
    "    \"year\", \"month\", \"day\", \"carrier\", \"airport\", \n",
    "    \"arr_flights\", \"arr_del15\", \"carrier_ct\", \"weather_ct\", \n",
    "    \"nas_ct\", \"security_ct\", \"late_aircraft_ct\", \"arr_cancelled\", \n",
    "    \"arr_diverted\", \"arr_delay\", \"carrier_delay\", \"weather_delay\", \n",
    "    \"nas_delay\", \"security_delay\", \"late_aircraft_delay\", \"delay_rate\"\n",
    "]\n",
    "\n",
    "# ✅ Generate random fake data\n",
    "def generate_fake_data(num_rows=500):\n",
    "    fake_data = []\n",
    "    for _ in range(num_rows):\n",
    "        fake_row = [\n",
    "            random.randint(2000, 2023),  # year\n",
    "            random.randint(1, 12),  # month\n",
    "            random.randint(1, 28),  # day\n",
    "            random.choice([\"AA\", \"DL\", \"UA\", \"SW\", \"AS\", \"EV\"]),  # carrier\n",
    "            random.choice([\"JFK\", \"LAX\", \"ORD\", \"ATL\", \"DFW\", \"DEN\"]),  # airport\n",
    "            random.randint(50, 1000),  # arr_flights\n",
    "            random.randint(0, 200),  # arr_del15\n",
    "            random.randint(0, 50),  # carrier_ct\n",
    "            random.randint(0, 50),  # weather_ct\n",
    "            random.randint(0, 50),  # nas_ct\n",
    "            random.randint(0, 50),  # security_ct\n",
    "            random.randint(0, 50),  # late_aircraft_ct\n",
    "            random.randint(0, 10),  # arr_cancelled\n",
    "            random.randint(0, 10),  # arr_diverted\n",
    "            random.uniform(0, 300),  # arr_delay\n",
    "            random.uniform(0, 100),  # carrier_delay\n",
    "            random.uniform(0, 100),  # weather_delay\n",
    "            random.uniform(0, 100),  # nas_delay\n",
    "            random.uniform(0, 100),  # security_delay\n",
    "            random.uniform(0, 100),  # late_aircraft_delay\n",
    "            random.uniform(1.0, 2.0),  # delay_rate\n",
    "        ]\n",
    "        fake_data.append(fake_row)\n",
    "    return pd.DataFrame(fake_data, columns=columns)\n",
    "\n",
    "# ✅ Write fake data to CSV\n",
    "fake_data = generate_fake_data(500)  # Generate 500 records\n",
    "fake_data_file = \"fake_flight_data.csv\"\n",
    "fake_data.to_csv(fake_data_file, index=False, header=False)\n",
    "print(f\"✅ Fake data generated and saved to {fake_data_file}\")\n",
    "\n",
    "# ✅ Load fake data for inference\n",
    "fake_data = pd.read_csv(fake_data_file, header=None)\n",
    "\n",
    "# ✅ Apply Label Encoding to `carrier` and `airport`\n",
    "label_encoders = {}\n",
    "for col_index, col_name in enumerate([\"carrier\", \"airport\"]):  \n",
    "    le = LabelEncoder()\n",
    "    fake_data[col_index + 3] = le.fit_transform(fake_data[col_index + 3])  # Encoding categorical columns\n",
    "\n",
    "# ✅ Convert to CSV format for inference\n",
    "fake_data_str_list = fake_data.apply(lambda row: \",\".join(row.astype(str)), axis=1).tolist()\n",
    "\n",
    "# ✅ Initialize predictor\n",
    "predictor = Predictor(\n",
    "    endpoint_name=\"flight-delay-xgboost-endpoint-single-request\",\n",
    "    sagemaker_session=sagemaker.Session(),\n",
    "    serializer=CSVSerializer(),\n",
    ")\n",
    "\n",
    "# ✅ Send Fake Data for Inference (Print Only First 5)\n",
    "responses = []\n",
    "for i, row_str in enumerate(fake_data_str_list):\n",
    "    response = predictor.predict(row_str)\n",
    "    responses.append(response)\n",
    "\n",
    "    # ✅ Print only the first 5 for visibility\n",
    "    if i < 5:\n",
    "        print(f\"🟢 Input: {row_str}\")\n",
    "        print(f\"🔵 Prediction: {response}\")\n",
    "\n",
    "print(f\"✅ Sent {len(fake_data_str_list)} rows to the endpoint. Check CloudWatch for drift alerts.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "81081732-4726-4802-843e-006c7697dd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🟢 Monitoring Schedule: FlightDelayMonitor\n",
      "   - Status: Scheduled\n",
      "   - Last Run: No runs yet\n",
      "   - Next Run: Scheduled every hour\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# ✅ Initialize SageMaker client\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "# ✅ Retrieve monitoring schedule details\n",
    "schedule_name = \"FlightDelayMonitor\"\n",
    "response = sm_client.describe_monitoring_schedule(MonitoringScheduleName=schedule_name)\n",
    "\n",
    "# ✅ Print status\n",
    "print(f\"🟢 Monitoring Schedule: {schedule_name}\")\n",
    "print(f\"   - Status: {response['MonitoringScheduleStatus']}\")\n",
    "print(f\"   - Last Run: {response.get('LastMonitoringExecutionSummary', {}).get('CreationTime', 'No runs yet')}\")\n",
    "print(f\"   - Next Run: Scheduled every hour\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed43b7c-26ae-4a0c-a147-5eef717c106c",
   "metadata": {},
   "source": [
    "# End of Feature Drift Monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd87ddb-3866-4011-bc10-a6f7461ab9af",
   "metadata": {},
   "source": [
    "# Data Quality Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "dc0870c1-790a-4617-84f8-45ed2902bceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: .\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Results Path: s3://sagemaker-us-east-1-607916531205/baseline_results/\n",
      "Monitoring Results Path: s3://sagemaker-us-east-1-607916531205/monitoring_results/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.model_monitor.model_monitoring:Creating Monitoring Schedule with name: FlightDelayDataQualityMonitor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Monitoring schedule created for endpoint: flight-delay-xgboost-endpoint-single-request\n",
      "Stored 'flight_delay_data_quality_monitor_schedule_name' (str)\n",
      "✅ Stored Data Quality monitor schedule name in %store.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.model_monitor import DefaultModelMonitor, CronExpressionGenerator, EndpointInput\n",
    "\n",
    "# ✅ Define S3 paths\n",
    "monitoring_results_uri = f\"s3://{bucket}/monitoring_results/\"\n",
    "\n",
    "print(f\"Baseline Results Path: {baseline_results_uri}\")\n",
    "print(f\"Monitoring Results Path: {monitoring_results_uri}\")\n",
    "\n",
    "# ✅ Configure the model monitor (Data Quality)\n",
    "model_monitor = DefaultModelMonitor(\n",
    "    role=role,  # ✅ Use role from %store\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    max_runtime_in_seconds=3600,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "# ✅ Define Endpoint Input Correctly (without dataset_format)\n",
    "endpoint_input = EndpointInput(\n",
    "    endpoint_name=\"flight-delay-xgboost-endpoint-single-request\",\n",
    "    destination=\"/opt/ml/processing/input\"\n",
    ")\n",
    "\n",
    "# ✅ Monitoring Schedule Name\n",
    "flight_delay_data_quality_monitor_schedule_name = \"FlightDelayDataQualityMonitor\"\n",
    "\n",
    "# ✅ Check if the schedule already exists (to avoid errors)\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "existing_schedules = sm_client.list_monitoring_schedules()[\"MonitoringScheduleSummaries\"]\n",
    "schedule_names = [schedule[\"MonitoringScheduleName\"] for schedule in existing_schedules]\n",
    "\n",
    "if flight_delay_data_quality_monitor_schedule_name in schedule_names:\n",
    "    print(f\"⚠️ Monitoring schedule '{flight_delay_data_quality_monitor_schedule_name}' already exists. Skipping creation.\")\n",
    "else:\n",
    "    # ✅ Schedule Data Monitoring (Every Hour)\n",
    "    model_monitor.create_monitoring_schedule(\n",
    "        monitor_schedule_name=flight_delay_data_quality_monitor_schedule_name,\n",
    "        endpoint_input=endpoint_input,  # ✅ Properly pass EndpointInput\n",
    "        output_s3_uri=monitoring_results_uri,\n",
    "        schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "        enable_cloudwatch_metrics=True,\n",
    "    )\n",
    "    print(f\"✅ Monitoring schedule created for endpoint: flight-delay-xgboost-endpoint-single-request\")\n",
    "\n",
    "# ✅ Store the second monitor schedule name in %store\n",
    "%store flight_delay_data_quality_monitor_schedule_name\n",
    "print(\"✅ Stored Data Quality monitor schedule name in %store.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a306c573-33b0-47c2-8ad4-1e3eced3467c",
   "metadata": {},
   "source": [
    "## Send Fake Data To Trigger Alarm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8f74b993-21f9-4e58-9e40-d73e0e8a6040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fake data with issues generated and saved to fake_flight_data_with_issues.csv\n",
      "🟢 Input: 2013.0,8.0,15.0,2.0,3.0,730.0,nan,49.0,nan,44.0,27.0,nan,7.0,3.0,214.91671084684117,10000.0,21.33190081813193,63.97851559474853,71.42439276207578,12.967931724957072,1.652889545147565\n",
      "🔵 Prediction: b'0.0022950877901166677\\n'\n",
      "🟢 Input: 2009.0,10.0,2.0,5.0,3.0,924.0,nan,42.0,8.0,27.0,38.0,nan,3.0,nan,157.8906414584232,14.401232785611771,2.009893107436067,96.31735185992108,48.21721503635166,6.516452910099446,1.5685100601887108\n",
      "🔵 Prediction: b'0.0022950877901166677\\n'\n",
      "🟢 Input: 2013.0,8.0,16.0,5.0,2.0,57.0,54.0,nan,50.0,11.0,nan,6.0,nan,nan,278.2355559052447,80.65517873108026,80.59374894485404,95.87863404295616,79.5469161478308,16.17224497405172,1.7182541234414552\n",
      "🔵 Prediction: b'1.0346633644076064e-05\\n'\n",
      "🟢 Input: 2009.0,4.0,12.0,1.0,2.0,942.0,8.0,nan,nan,nan,34.0,44.0,7.0,nan,206.40601430319973,33.28253542549483,68.47700015995088,36.32202356467392,94.61250398637748,92.75321425830704,1.9417928078546056\n",
      "🔵 Prediction: b'4.337779500929173e-06\\n'\n",
      "🟢 Input: 2002.0,3.0,7.0,5.0,5.0,944.0,nan,nan,17.0,38.0,nan,nan,nan,nan,82.41684599665243,43.51986854086864,15.06533478890779,4.916432437357921,67.75988197226505,36.0591183308158,1.0817069927170762\n",
      "🔵 Prediction: b'0.9978665709495544\\n'\n",
      "✅ Sent 100 rows to the endpoint. Check CloudWatch for drift alerts.\n"
     ]
    }
   ],
   "source": [
    "def generate_extreme_fake_data(num_rows=100):\n",
    "    fake_data = []\n",
    "    for _ in range(num_rows):\n",
    "        fake_row = [\n",
    "            random.randint(2000, 2023),  # year\n",
    "            random.randint(1, 12),  # month\n",
    "            random.randint(1, 28),  # day\n",
    "            random.choice([\"AA\", \"DL\", \"UA\", \"SW\", \"AS\", \"EV\"]),  # carrier\n",
    "            random.choice([\"JFK\", \"LAX\", \"ORD\", \"ATL\", \"DFW\", \"DEN\"]),  # airport\n",
    "            random.randint(50, 1000),  # arr_flights\n",
    "            random.choice([None, random.randint(0, 200)]),  # arr_del15 (introduce None)\n",
    "            random.choice([None, random.randint(0, 50)]),  # carrier_ct (introduce None)\n",
    "            random.choice([None, random.randint(0, 50)]),  # weather_ct (introduce None)\n",
    "            random.choice([None, random.randint(0, 50)]),  # nas_ct (introduce None)\n",
    "            random.choice([None, random.randint(0, 50)]),  # security_ct (introduce None)\n",
    "            random.choice([None, random.randint(0, 50)]),  # late_aircraft_ct (introduce None)\n",
    "            random.choice([None, random.randint(0, 10)]),  # arr_cancelled (introduce None)\n",
    "            random.choice([None, random.randint(0, 10)]),  # arr_diverted (introduce None)\n",
    "            random.uniform(0, 300),  # arr_delay\n",
    "            random.uniform(0, 100),  # carrier_delay\n",
    "            random.uniform(0, 100),  # weather_delay\n",
    "            random.uniform(0, 100),  # nas_delay\n",
    "            random.uniform(0, 100),  # security_delay\n",
    "            random.uniform(0, 100),  # late_aircraft_delay\n",
    "            random.uniform(1.0, 2.0),  # delay_rate\n",
    "        ]\n",
    "        \n",
    "        # Introduce extreme outliers (more extreme than before)\n",
    "        if random.random() > 0.9:\n",
    "            fake_row[15] = 10000  # Extreme value for carrier_delay (outlier)\n",
    "        \n",
    "        fake_data.append(fake_row)\n",
    "    return pd.DataFrame(fake_data, columns=columns)\n",
    "\n",
    "# Generate extreme fake data with issues\n",
    "fake_data_with_issues = generate_extreme_fake_data(100)  # Generate 100 rows with extreme issues\n",
    "fake_data_file_with_issues = \"fake_flight_data_with_issues.csv\"\n",
    "fake_data_with_issues.to_csv(fake_data_file_with_issues, index=False, header=False)\n",
    "print(f\"✅ Fake data with issues generated and saved to {fake_data_file_with_issues}\")\n",
    "\n",
    "# ✅ Load the generated data\n",
    "fake_data_with_issues = pd.read_csv(fake_data_file_with_issues, header=None)\n",
    "\n",
    "# ✅ Apply label encoding\n",
    "label_encoders = {}\n",
    "for col_index, col_name in enumerate([\"carrier\", \"airport\"]):  \n",
    "    le = LabelEncoder()\n",
    "    fake_data_with_issues[col_index + 3] = le.fit_transform(fake_data_with_issues[col_index + 3])  # Encoding categorical columns\n",
    "\n",
    "# ✅ Convert to CSV format for inference\n",
    "fake_data_str_list = fake_data_with_issues.apply(lambda row: \",\".join(row.astype(str)), axis=1).tolist()\n",
    "\n",
    "# ✅ Send the data to the endpoint\n",
    "predictor = Predictor(\n",
    "    endpoint_name=\"flight-delay-xgboost-endpoint-single-request\",\n",
    "    sagemaker_session=sagemaker.Session(),\n",
    "    serializer=CSVSerializer(),\n",
    ")\n",
    "\n",
    "responses = []\n",
    "for i, row_str in enumerate(fake_data_str_list):\n",
    "    response = predictor.predict(row_str)\n",
    "    responses.append(response)\n",
    "    if i < 5:\n",
    "        print(f\"🟢 Input: {row_str}\")\n",
    "        print(f\"🔵 Prediction: {response}\")\n",
    "\n",
    "print(f\"✅ Sent {len(fake_data_str_list)} rows to the endpoint. Check CloudWatch for drift alerts.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f09167-cada-4cd9-81e0-d7a9855c6fdb",
   "metadata": {},
   "source": [
    "# End of Data Quality Monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fef4be-e00c-43d9-9bff-e551e08f774c",
   "metadata": {},
   "source": [
    "# Infrastructure Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d576d01d-3c89-4528-825d-a24b73adcf21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Infrastructure monitoring alarms for latency and failures created successfully.\n",
      "Stored 'alarm_name_latency' (str)\n",
      "Stored 'alarm_name_failure' (str)\n",
      "Stored 'latency_threshold' (float)\n",
      "Stored 'failure_threshold' (float)\n",
      "✅ Stored infrastructure monitoring data in %store.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# ✅ Initialize CloudWatch client\n",
    "cw_client = boto3.client(\"cloudwatch\")\n",
    "\n",
    "# ✅ Define alarm names and thresholds\n",
    "alarm_name_latency = \"FlightDelayEndpointLatencyAlarm\"\n",
    "alarm_name_failure = \"FlightDelayEndpointFailureAlarm\"\n",
    "latency_threshold = 2.0  # 2 seconds for latency alarm\n",
    "failure_threshold = 0.05  # 5% failure rate\n",
    "\n",
    "# ✅ Create Alarm for High Latency (e.g., if response time exceeds 2 seconds)\n",
    "response_latency = cw_client.put_metric_alarm(\n",
    "    AlarmName=alarm_name_latency,\n",
    "    AlarmDescription=\"Triggers when endpoint latency exceeds 2 seconds.\",\n",
    "    ActionsEnabled=True,\n",
    "    MetricName=\"ModelLatency\",\n",
    "    Namespace=\"aws/sagemaker/Endpoints\",\n",
    "    Statistic=\"Average\",\n",
    "    Dimensions=[\n",
    "        {\"Name\": \"Endpoint\", \"Value\": \"flight-delay-xgboost-endpoint-single-request\"}\n",
    "    ],\n",
    "    Period=60,  # Check every minute\n",
    "    EvaluationPeriods=1,\n",
    "    DatapointsToAlarm=1,\n",
    "    Threshold=latency_threshold,  # 2 seconds\n",
    "    ComparisonOperator=\"GreaterThanThreshold\",\n",
    "    TreatMissingData=\"notBreaching\"\n",
    ")\n",
    "\n",
    "# ✅ Create Alarm for Invocation Failures (e.g., if failures exceed 5%)\n",
    "response_failure = cw_client.put_metric_alarm(\n",
    "    AlarmName=alarm_name_failure,\n",
    "    AlarmDescription=\"Triggers when endpoint invocation failures exceed 5%.\",\n",
    "    ActionsEnabled=True,\n",
    "    MetricName=\"InvocationFailures\",\n",
    "    Namespace=\"aws/sagemaker/Endpoints\",\n",
    "    Statistic=\"Sum\",\n",
    "    Dimensions=[\n",
    "        {\"Name\": \"Endpoint\", \"Value\": \"flight-delay-xgboost-endpoint-single-request\"}\n",
    "    ],\n",
    "    Period=60,  # Check every minute\n",
    "    EvaluationPeriods=1,\n",
    "    DatapointsToAlarm=1,\n",
    "    Threshold=failure_threshold,  # 5% failure rate\n",
    "    ComparisonOperator=\"GreaterThanThreshold\",\n",
    "    TreatMissingData=\"notBreaching\"\n",
    ")\n",
    "\n",
    "print(\"✅ Infrastructure monitoring alarms for latency and failures created successfully.\")\n",
    "\n",
    "# ✅ Store infrastructure monitoring data in %store\n",
    "%store alarm_name_latency\n",
    "%store alarm_name_failure\n",
    "%store latency_threshold\n",
    "%store failure_threshold\n",
    "\n",
    "print(\"✅ Stored infrastructure monitoring data in %store.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a2c81a-376f-4f25-bfe1-73934c8cf0ca",
   "metadata": {},
   "source": [
    "# Clean up script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b6e8d75-f265-43cc-9bbe-f33541c07d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing monitoring schedules for endpoint: flight-delay-xgboost-endpoint-single-request\n",
      "Deleting monitoring schedule: FlightDelayMonitor\n",
      "All monitoring schedules deleted.\n",
      "Deleting endpoint: flight-delay-xgboost-endpoint-single-request\n",
      "Endpoint 'flight-delay-xgboost-endpoint-single-request' deleted.\n",
      "Deleting endpoint configuration: flight-delay-xgboost-endpoint-single-request-config\n",
      "Error while deleting endpoint configuration: An error occurred (ValidationException) when calling the DeleteEndpointConfig operation: Could not find endpoint configuration \"flight-delay-xgboost-endpoint-single-request-config\".\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ValidationException) when calling the DeleteEndpointConfig operation: Could not find endpoint configuration \"flight-delay-xgboost-endpoint-single-request-config\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeleting endpoint configuration: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mendpoint_config_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 43\u001b[0m     \u001b[43msagemaker_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelete_endpoint_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEndpointConfigName\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint_config_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEndpoint configuration \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mendpoint_config_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m deleted.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m sagemaker_client\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mResourceNotFound \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/client.py:569\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    566\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m     )\n\u001b[1;32m    568\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 569\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/client.py:1023\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1020\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1021\u001b[0m     )\n\u001b[1;32m   1022\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1023\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ValidationException) when calling the DeleteEndpointConfig operation: Could not find endpoint configuration \"flight-delay-xgboost-endpoint-single-request-config\"."
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from time import sleep\n",
    "\n",
    "# Initialize SageMaker client\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "# Define resources to clean up\n",
    "endpoint_name = \"flight-delay-xgboost-endpoint-single-request\"\n",
    "monitoring_schedule_name = \"ModelMonitorForXGBoost\"  # Adjust if the schedule name is different\n",
    "endpoint_config_name = f\"{endpoint_name}-config\"\n",
    "\n",
    "# Step 1: Delete Monitoring Schedules\n",
    "try:\n",
    "    # List all monitoring schedules associated with the endpoint\n",
    "    print(f\"Listing monitoring schedules for endpoint: {endpoint_name}\")\n",
    "    monitoring_schedules = sagemaker_client.list_monitoring_schedules()['MonitoringScheduleSummaries']\n",
    "    for schedule in monitoring_schedules:\n",
    "        if schedule['EndpointName'] == endpoint_name:\n",
    "            print(f\"Deleting monitoring schedule: {schedule['MonitoringScheduleName']}\")\n",
    "            sagemaker_client.delete_monitoring_schedule(MonitoringScheduleName=schedule['MonitoringScheduleName'])\n",
    "            sleep(10)  # Allow time for the monitoring schedule deletion process\n",
    "    print(\"All monitoring schedules deleted.\")\n",
    "except sagemaker_client.exceptions.ResourceNotFound as e:\n",
    "    print(\"Monitoring schedule not found. Skipping deletion.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error while deleting monitoring schedules: {e}\")\n",
    "    raise\n",
    "\n",
    "# Step 2: Delete Endpoint\n",
    "try:\n",
    "    print(f\"Deleting endpoint: {endpoint_name}\")\n",
    "    sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "    print(f\"Endpoint '{endpoint_name}' deleted.\")\n",
    "except sagemaker_client.exceptions.ResourceNotFound as e:\n",
    "    print(f\"Endpoint '{endpoint_name}' not found. Skipping deletion.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error while deleting endpoint: {e}\")\n",
    "    raise\n",
    "\n",
    "# Step 3: Delete Endpoint Configuration\n",
    "try:\n",
    "    print(f\"Deleting endpoint configuration: {endpoint_config_name}\")\n",
    "    sagemaker_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "    print(f\"Endpoint configuration '{endpoint_config_name}' deleted.\")\n",
    "except sagemaker_client.exceptions.ResourceNotFound as e:\n",
    "    print(f\"Endpoint configuration '{endpoint_config_name}' not found. Skipping deletion.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error while deleting endpoint configuration: {e}\")\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
